2024-06-22 08:47:55,694 ==================================================
2024-06-22 08:47:55,694 Training with Guassian Noise of 1 length...
2024-06-22 08:47:57,208 Epoch [1/8], Batch [1/748], Loss: 1.6309
2024-06-22 08:48:21,592 Epoch [1/8], Batch [51/748], Loss: 1.5795
2024-06-22 08:48:46,236 Epoch [1/8], Batch [101/748], Loss: 1.5978
2024-06-22 08:49:10,617 Epoch [1/8], Batch [151/748], Loss: 1.6197
2024-06-22 08:49:35,402 Epoch [1/8], Batch [201/748], Loss: 1.6121
2024-06-22 08:50:00,258 Epoch [1/8], Batch [251/748], Loss: 1.5556
2024-06-22 08:50:24,930 Epoch [1/8], Batch [301/748], Loss: 1.5435
2024-06-22 08:50:50,301 Epoch [1/8], Batch [351/748], Loss: 1.5389
2024-06-22 08:51:15,021 Epoch [1/8], Batch [401/748], Loss: 1.5790
2024-06-22 08:51:39,958 Epoch [1/8], Batch [451/748], Loss: 1.6348
2024-06-22 08:52:04,515 Epoch [1/8], Batch [501/748], Loss: 1.6184
2024-06-22 08:52:29,427 Epoch [1/8], Batch [551/748], Loss: 1.5580
2024-06-22 08:52:54,529 Epoch [1/8], Batch [601/748], Loss: 1.5328
2024-06-22 08:53:19,452 Epoch [1/8], Batch [651/748], Loss: 1.5003
2024-06-22 08:53:44,396 Epoch [1/8], Batch [701/748], Loss: 1.5472
2024-06-22 08:54:07,430 Epoch 1/8, Train Loss: 1.5753, Train Accuracy: 0.2667
2024-06-22 08:55:37,175 Epoch 1/8, Val Loss: 1.5333, Val Accuracy: 0.3125
2024-06-22 08:55:37,711 Epoch [2/8], Batch [1/748], Loss: 1.4685
2024-06-22 08:56:02,204 Epoch [2/8], Batch [51/748], Loss: 1.5072
2024-06-22 08:56:27,071 Epoch [2/8], Batch [101/748], Loss: 1.5637
2024-06-22 08:56:52,025 Epoch [2/8], Batch [151/748], Loss: 1.5169
2024-06-22 08:57:16,875 Epoch [2/8], Batch [201/748], Loss: 1.5803
2024-06-22 08:57:41,525 Epoch [2/8], Batch [251/748], Loss: 1.5357
2024-06-22 08:58:06,153 Epoch [2/8], Batch [301/748], Loss: 1.5761
2024-06-22 08:58:31,291 Epoch [2/8], Batch [351/748], Loss: 1.4746
2024-06-22 08:58:56,043 Epoch [2/8], Batch [401/748], Loss: 1.5553
2024-06-22 08:59:21,006 Epoch [2/8], Batch [451/748], Loss: 1.5120
2024-06-22 08:59:46,093 Epoch [2/8], Batch [501/748], Loss: 1.5364
2024-06-22 09:00:10,985 Epoch [2/8], Batch [551/748], Loss: 1.5653
2024-06-22 09:00:35,700 Epoch [2/8], Batch [601/748], Loss: 1.5129
2024-06-22 09:01:00,490 Epoch [2/8], Batch [651/748], Loss: 1.6044
2024-06-22 09:01:25,480 Epoch [2/8], Batch [701/748], Loss: 1.3962
2024-06-22 09:01:48,702 Epoch 2/8, Train Loss: 1.5279, Train Accuracy: 0.3305
2024-06-22 09:03:18,389 Epoch 2/8, Val Loss: 1.4867, Val Accuracy: 0.3690
2024-06-22 09:03:18,870 Epoch [3/8], Batch [1/748], Loss: 1.5102
2024-06-22 09:03:43,153 Epoch [3/8], Batch [51/748], Loss: 1.5210
2024-06-22 09:04:07,652 Epoch [3/8], Batch [101/748], Loss: 1.5707
2024-06-22 09:04:32,263 Epoch [3/8], Batch [151/748], Loss: 1.6005
2024-06-22 09:04:56,787 Epoch [3/8], Batch [201/748], Loss: 1.3900
2024-06-22 09:05:21,421 Epoch [3/8], Batch [251/748], Loss: 1.5557
2024-06-22 09:05:45,767 Epoch [3/8], Batch [301/748], Loss: 1.5165
2024-06-22 09:06:09,941 Epoch [3/8], Batch [351/748], Loss: 1.5254
2024-06-22 09:06:34,495 Epoch [3/8], Batch [401/748], Loss: 1.5179
2024-06-22 09:06:59,090 Epoch [3/8], Batch [451/748], Loss: 1.4401
2024-06-22 09:07:23,453 Epoch [3/8], Batch [501/748], Loss: 1.4805
2024-06-22 09:07:48,121 Epoch [3/8], Batch [551/748], Loss: 1.4671
2024-06-22 09:08:12,449 Epoch [3/8], Batch [601/748], Loss: 1.5289
2024-06-22 09:08:37,292 Epoch [3/8], Batch [651/748], Loss: 1.4655
2024-06-22 09:09:01,645 Epoch [3/8], Batch [701/748], Loss: 1.4286
2024-06-22 09:09:24,380 Epoch 3/8, Train Loss: 1.4934, Train Accuracy: 0.3668
2024-06-22 09:10:53,497 Epoch 3/8, Val Loss: 1.4457, Val Accuracy: 0.4125
2024-06-22 09:10:53,976 Epoch [4/8], Batch [1/748], Loss: 1.4247
2024-06-22 09:11:18,486 Epoch [4/8], Batch [51/748], Loss: 1.4692
2024-06-22 09:11:43,071 Epoch [4/8], Batch [101/748], Loss: 1.5225
2024-06-22 09:12:07,777 Epoch [4/8], Batch [151/748], Loss: 1.4436
2024-06-22 09:28:08,256 ==================================================
2024-06-22 09:28:08,256 Training with Guassian Noise of 1 length...
2024-06-22 09:35:51,621 ==================================================
2024-06-22 09:35:51,622 Training with Guassian Noise of 1 length...
2024-06-22 09:39:27,972 ==================================================
2024-06-22 09:39:27,972 Training with Guassian Noise of 1 length...
2024-06-22 09:39:30,006 Epoch [1/8], Batch [1/374], Loss: 1.6544
2024-06-22 09:40:21,090 Epoch [1/8], Batch [51/374], Loss: 1.4753
2024-06-22 09:41:12,641 Epoch [1/8], Batch [101/374], Loss: 1.2630
2024-06-22 09:42:04,852 Epoch [1/8], Batch [151/374], Loss: 1.1806
2024-06-22 09:42:57,365 Epoch [1/8], Batch [201/374], Loss: 1.1001
2024-06-22 09:43:49,409 Epoch [1/8], Batch [251/374], Loss: 1.0324
2024-06-22 09:44:41,668 Epoch [1/8], Batch [301/374], Loss: 1.1968
2024-06-22 09:45:33,814 Epoch [1/8], Batch [351/374], Loss: 0.9831
2024-06-22 09:45:57,725 Epoch 1/8, Train Loss: 1.2087, Train Accuracy: 0.4827
2024-06-22 09:46:43,414 Epoch 1/8, Val Loss: 1.0383, Val Accuracy: 0.5462
2024-06-22 09:46:44,482 Epoch [2/8], Batch [1/374], Loss: 0.9405
2024-06-22 09:47:36,603 Epoch [2/8], Batch [51/374], Loss: 0.8878
2024-06-22 09:48:28,589 Epoch [2/8], Batch [101/374], Loss: 0.8520
2024-06-22 09:49:20,340 Epoch [2/8], Batch [151/374], Loss: 0.9065
2024-06-22 09:50:12,298 Epoch [2/8], Batch [201/374], Loss: 0.8695
2024-06-22 09:51:04,395 Epoch [2/8], Batch [251/374], Loss: 1.0106
2024-06-22 09:51:56,263 Epoch [2/8], Batch [301/374], Loss: 0.9979
2024-06-22 09:52:48,179 Epoch [2/8], Batch [351/374], Loss: 0.8872
2024-06-22 09:53:11,913 Epoch 2/8, Train Loss: 0.9080, Train Accuracy: 0.6056
2024-06-22 09:53:56,965 Epoch 2/8, Val Loss: 0.9047, Val Accuracy: 0.6110
2024-06-22 09:53:58,005 Epoch [3/8], Batch [1/374], Loss: 0.7214
2024-06-22 09:54:50,096 Epoch [3/8], Batch [51/374], Loss: 1.0209
2024-06-22 09:55:42,176 Epoch [3/8], Batch [101/374], Loss: 0.7839
2024-06-22 09:56:34,374 Epoch [3/8], Batch [151/374], Loss: 0.8576
2024-06-22 09:57:26,389 Epoch [3/8], Batch [201/374], Loss: 0.7624
2024-06-22 09:58:18,153 Epoch [3/8], Batch [251/374], Loss: 0.7954
2024-06-22 09:59:10,094 Epoch [3/8], Batch [301/374], Loss: 0.7891
2024-06-22 10:00:01,629 Epoch [3/8], Batch [351/374], Loss: 0.7038
2024-06-22 10:00:25,272 Epoch 3/8, Train Loss: 0.7945, Train Accuracy: 0.6516
2024-06-22 10:01:10,062 Epoch 3/8, Val Loss: 0.8982, Val Accuracy: 0.6187
2024-06-22 10:01:11,069 Epoch [4/8], Batch [1/374], Loss: 1.0155
2024-06-22 10:02:02,640 Epoch [4/8], Batch [51/374], Loss: 0.9558
2024-06-22 10:02:54,895 Epoch [4/8], Batch [101/374], Loss: 0.6274
2024-06-22 10:03:46,419 Epoch [4/8], Batch [151/374], Loss: 0.7256
2024-06-22 10:04:38,223 Epoch [4/8], Batch [201/374], Loss: 0.6209
2024-06-22 10:05:30,540 Epoch [4/8], Batch [251/374], Loss: 0.6366
2024-06-22 10:06:22,708 Epoch [4/8], Batch [301/374], Loss: 0.9544
2024-06-22 10:07:14,968 Epoch [4/8], Batch [351/374], Loss: 0.6582
2024-06-22 10:07:38,537 Epoch 4/8, Train Loss: 0.6900, Train Accuracy: 0.6963
2024-06-22 10:08:23,875 Epoch 4/8, Val Loss: 0.8797, Val Accuracy: 0.6207
2024-06-22 10:08:24,921 Epoch [5/8], Batch [1/374], Loss: 0.6220
2024-06-22 10:09:16,650 Epoch [5/8], Batch [51/374], Loss: 0.6476
2024-06-22 10:10:08,586 Epoch [5/8], Batch [101/374], Loss: 0.5935
2024-06-22 10:11:01,151 Epoch [5/8], Batch [151/374], Loss: 0.6544
2024-06-22 10:11:53,763 Epoch [5/8], Batch [201/374], Loss: 0.4186
2024-06-22 10:12:46,377 Epoch [5/8], Batch [251/374], Loss: 0.6522
2024-06-22 10:13:39,047 Epoch [5/8], Batch [301/374], Loss: 0.4577
2024-06-22 10:14:31,664 Epoch [5/8], Batch [351/374], Loss: 0.6949
2024-06-22 10:14:55,764 Epoch 5/8, Train Loss: 0.6106, Train Accuracy: 0.7309
2024-06-22 10:15:42,082 Epoch 5/8, Val Loss: 0.8694, Val Accuracy: 0.6445
2024-06-22 10:15:43,137 Epoch [6/8], Batch [1/374], Loss: 0.6854
2024-06-22 10:16:35,265 Epoch [6/8], Batch [51/374], Loss: 0.2833
2024-06-22 10:17:27,469 Epoch [6/8], Batch [101/374], Loss: 0.4719
2024-06-22 10:18:19,807 Epoch [6/8], Batch [151/374], Loss: 0.5602
2024-06-22 10:19:13,073 Epoch [6/8], Batch [201/374], Loss: 0.5186
2024-06-22 10:20:06,435 Epoch [6/8], Batch [251/374], Loss: 0.6467
2024-06-22 10:20:59,852 Epoch [6/8], Batch [301/374], Loss: 0.5155
2024-06-22 10:21:52,940 Epoch [6/8], Batch [351/374], Loss: 0.5937
2024-06-22 10:22:17,192 Epoch 6/8, Train Loss: 0.5179, Train Accuracy: 0.7774
2024-06-22 10:23:04,916 Epoch 6/8, Val Loss: 0.9833, Val Accuracy: 0.6334
2024-06-22 10:23:05,974 Epoch [7/8], Batch [1/374], Loss: 0.4329
2024-06-22 10:23:59,079 Epoch [7/8], Batch [51/374], Loss: 0.3831
2024-06-22 10:24:52,366 Epoch [7/8], Batch [101/374], Loss: 0.3174
2024-06-22 10:25:45,716 Epoch [7/8], Batch [151/374], Loss: 0.3843
2024-06-22 10:26:38,916 Epoch [7/8], Batch [201/374], Loss: 0.2852
2024-06-22 10:27:31,791 Epoch [7/8], Batch [251/374], Loss: 0.2905
2024-06-22 10:28:24,290 Epoch [7/8], Batch [301/374], Loss: 0.4573
2024-06-22 10:29:17,181 Epoch [7/8], Batch [351/374], Loss: 0.2526
2024-06-22 10:29:41,261 Epoch 7/8, Train Loss: 0.4276, Train Accuracy: 0.8222
2024-06-22 10:30:27,916 Epoch 7/8, Val Loss: 1.0477, Val Accuracy: 0.6107
2024-06-22 10:30:28,959 Epoch [8/8], Batch [1/374], Loss: 0.2861
2024-06-22 10:31:21,522 Epoch [8/8], Batch [51/374], Loss: 0.2371
2024-06-22 10:32:14,020 Epoch [8/8], Batch [101/374], Loss: 0.3257
2024-06-22 10:33:07,129 Epoch [8/8], Batch [151/374], Loss: 0.4387
2024-06-22 10:33:59,821 Epoch [8/8], Batch [201/374], Loss: 0.4315
2024-06-22 10:34:52,840 Epoch [8/8], Batch [251/374], Loss: 0.4331
2024-06-22 10:35:45,371 Epoch [8/8], Batch [301/374], Loss: 0.5547
2024-06-22 10:36:38,106 Epoch [8/8], Batch [351/374], Loss: 0.4141
2024-06-22 10:37:02,027 Epoch 8/8, Train Loss: 0.3651, Train Accuracy: 0.8549
2024-06-22 10:37:48,476 Epoch 8/8, Val Loss: 1.0186, Val Accuracy: 0.6405
2024-06-22 10:37:48,477 Training finished!
2024-06-22 10:37:48,477 ==================================================
2024-06-22 10:37:49,108 ==================================================
2024-06-22 10:37:49,108 Training with Guassian Noise of 16 length...
2024-06-22 10:37:50,204 Epoch [1/8], Batch [1/374], Loss: 1.6261
2024-06-22 10:38:42,877 Epoch [1/8], Batch [51/374], Loss: 1.4978
2024-06-22 10:39:35,609 Epoch [1/8], Batch [101/374], Loss: 1.2382
2024-06-22 10:40:28,505 Epoch [1/8], Batch [151/374], Loss: 1.0523
2024-06-22 10:41:21,270 Epoch [1/8], Batch [201/374], Loss: 1.0712
2024-06-22 10:42:14,119 Epoch [1/8], Batch [251/374], Loss: 1.0781
2024-06-22 10:43:07,100 Epoch [1/8], Batch [301/374], Loss: 0.8509
2024-06-22 10:43:59,786 Epoch [1/8], Batch [351/374], Loss: 1.1207
2024-06-22 10:44:23,984 Epoch 1/8, Train Loss: 1.1717, Train Accuracy: 0.5125
2024-06-22 10:45:10,233 Epoch 1/8, Val Loss: 0.9480, Val Accuracy: 0.5883
2024-06-22 10:45:11,276 Epoch [2/8], Batch [1/374], Loss: 0.9443
2024-06-22 10:46:03,854 Epoch [2/8], Batch [51/374], Loss: 0.8271
2024-06-22 10:46:56,567 Epoch [2/8], Batch [101/374], Loss: 0.6608
2024-06-22 10:47:49,316 Epoch [2/8], Batch [151/374], Loss: 0.9649
2024-06-22 10:48:42,421 Epoch [2/8], Batch [201/374], Loss: 0.9152
2024-06-22 10:49:35,958 Epoch [2/8], Batch [251/374], Loss: 0.7430
2024-06-22 10:50:28,676 Epoch [2/8], Batch [301/374], Loss: 0.9483
2024-06-22 10:51:21,525 Epoch [2/8], Batch [351/374], Loss: 0.8388
2024-06-22 10:51:45,614 Epoch 2/8, Train Loss: 0.8671, Train Accuracy: 0.6248
2024-06-22 10:52:32,405 Epoch 2/8, Val Loss: 0.9643, Val Accuracy: 0.5756
2024-06-22 10:52:33,474 Epoch [3/8], Batch [1/374], Loss: 0.7128
2024-06-22 10:53:26,190 Epoch [3/8], Batch [51/374], Loss: 0.6132
2024-06-22 10:54:19,057 Epoch [3/8], Batch [101/374], Loss: 0.6701
2024-06-22 10:55:11,918 Epoch [3/8], Batch [151/374], Loss: 0.7004
2024-06-22 10:56:04,867 Epoch [3/8], Batch [201/374], Loss: 0.6214
2024-06-22 10:56:57,796 Epoch [3/8], Batch [251/374], Loss: 0.7713
2024-06-22 10:57:50,567 Epoch [3/8], Batch [301/374], Loss: 0.9212
2024-06-22 10:58:43,935 Epoch [3/8], Batch [351/374], Loss: 0.8304
2024-06-22 10:59:07,983 Epoch 3/8, Train Loss: 0.7517, Train Accuracy: 0.6775
2024-06-22 10:59:54,663 Epoch 3/8, Val Loss: 0.9020, Val Accuracy: 0.6154
2024-06-22 10:59:55,688 Epoch [4/8], Batch [1/374], Loss: 0.7192
2024-06-22 11:00:48,369 Epoch [4/8], Batch [51/374], Loss: 0.9472
2024-06-22 11:01:40,895 Epoch [4/8], Batch [101/374], Loss: 0.6690
2024-06-22 11:02:33,942 Epoch [4/8], Batch [151/374], Loss: 0.6325
2024-06-22 11:03:26,814 Epoch [4/8], Batch [201/374], Loss: 0.4350
2024-06-22 11:04:19,565 Epoch [4/8], Batch [251/374], Loss: 0.7762
2024-06-22 11:05:12,556 Epoch [4/8], Batch [301/374], Loss: 0.6751
2024-06-22 11:06:05,496 Epoch [4/8], Batch [351/374], Loss: 0.7329
2024-06-22 11:06:29,830 Epoch 4/8, Train Loss: 0.6463, Train Accuracy: 0.7259
2024-06-22 11:07:16,800 Epoch 4/8, Val Loss: 0.8511, Val Accuracy: 0.6498
2024-06-22 11:07:17,832 Epoch [5/8], Batch [1/374], Loss: 0.4590
2024-06-22 11:08:10,637 Epoch [5/8], Batch [51/374], Loss: 0.6872
2024-06-22 11:09:03,586 Epoch [5/8], Batch [101/374], Loss: 0.6418
2024-06-22 11:09:56,256 Epoch [5/8], Batch [151/374], Loss: 0.7941
2024-06-22 11:10:49,027 Epoch [5/8], Batch [201/374], Loss: 0.5828
2024-06-22 11:11:42,133 Epoch [5/8], Batch [251/374], Loss: 0.4393
2024-06-22 11:12:35,078 Epoch [5/8], Batch [301/374], Loss: 0.6034
2024-06-22 11:13:28,210 Epoch [5/8], Batch [351/374], Loss: 0.6599
2024-06-22 11:13:52,326 Epoch 5/8, Train Loss: 0.5389, Train Accuracy: 0.7756
2024-06-22 11:14:39,291 Epoch 5/8, Val Loss: 0.9703, Val Accuracy: 0.6311
2024-06-22 11:14:40,361 Epoch [6/8], Batch [1/374], Loss: 0.4055
2024-06-22 11:15:33,122 Epoch [6/8], Batch [51/374], Loss: 0.4271
2024-06-22 11:16:25,852 Epoch [6/8], Batch [101/374], Loss: 0.3383
2024-06-22 11:17:18,969 Epoch [6/8], Batch [151/374], Loss: 0.3978
2024-06-22 11:18:12,079 Epoch [6/8], Batch [201/374], Loss: 0.4276
2024-06-22 11:19:05,263 Epoch [6/8], Batch [251/374], Loss: 0.7633
2024-06-22 11:19:58,326 Epoch [6/8], Batch [301/374], Loss: 0.4099
2024-06-22 11:20:51,127 Epoch [6/8], Batch [351/374], Loss: 0.5681
2024-06-22 11:21:15,173 Epoch 6/8, Train Loss: 0.4491, Train Accuracy: 0.8172
2024-06-22 11:22:01,585 Epoch 6/8, Val Loss: 0.9497, Val Accuracy: 0.6498
2024-06-22 11:22:02,625 Epoch [7/8], Batch [1/374], Loss: 0.3730
2024-06-22 11:22:55,581 Epoch [7/8], Batch [51/374], Loss: 0.3589
2024-06-22 11:23:48,219 Epoch [7/8], Batch [101/374], Loss: 0.2974
2024-06-22 11:24:40,994 Epoch [7/8], Batch [151/374], Loss: 0.3175
2024-06-22 11:25:33,918 Epoch [7/8], Batch [201/374], Loss: 0.2328
2024-06-22 11:26:27,030 Epoch [7/8], Batch [251/374], Loss: 0.2255
2024-06-22 11:27:19,970 Epoch [7/8], Batch [301/374], Loss: 0.3474
2024-06-22 11:28:12,520 Epoch [7/8], Batch [351/374], Loss: 0.3199
2024-06-22 11:28:36,629 Epoch 7/8, Train Loss: 0.3657, Train Accuracy: 0.8576
2024-06-22 11:29:23,202 Epoch 7/8, Val Loss: 0.9950, Val Accuracy: 0.6431
2024-06-22 11:29:24,271 Epoch [8/8], Batch [1/374], Loss: 0.2258
2024-06-22 11:30:16,899 Epoch [8/8], Batch [51/374], Loss: 0.2984
2024-06-22 11:31:09,912 Epoch [8/8], Batch [101/374], Loss: 0.2769
2024-06-22 11:32:02,645 Epoch [8/8], Batch [151/374], Loss: 0.2245
2024-06-22 11:32:55,539 Epoch [8/8], Batch [201/374], Loss: 0.2218
2024-06-22 11:33:48,316 Epoch [8/8], Batch [251/374], Loss: 0.2379
2024-06-22 11:34:41,516 Epoch [8/8], Batch [301/374], Loss: 0.3656
2024-06-22 11:35:34,025 Epoch [8/8], Batch [351/374], Loss: 0.2713
2024-06-22 11:35:57,888 Epoch 8/8, Train Loss: 0.2943, Train Accuracy: 0.8916
2024-06-22 11:36:44,242 Epoch 8/8, Val Loss: 1.1702, Val Accuracy: 0.6294
2024-06-22 11:36:44,243 Training finished!
2024-06-22 11:36:44,243 ==================================================
2024-06-22 11:36:44,922 ==================================================
2024-06-22 11:36:44,922 Training with Guassian Noise of 64 length...
2024-06-22 11:36:45,979 Epoch [1/8], Batch [1/374], Loss: 1.6007
2024-06-22 11:37:38,670 Epoch [1/8], Batch [51/374], Loss: 1.5464
2024-06-22 11:38:30,927 Epoch [1/8], Batch [101/374], Loss: 1.2754
2024-06-22 11:39:22,812 Epoch [1/8], Batch [151/374], Loss: 1.2330
2024-06-22 11:40:14,914 Epoch [1/8], Batch [201/374], Loss: 1.0710
2024-06-22 11:41:07,268 Epoch [1/8], Batch [251/374], Loss: 1.1359
2024-06-22 11:41:59,364 Epoch [1/8], Batch [301/374], Loss: 0.9784
2024-06-22 11:42:51,773 Epoch [1/8], Batch [351/374], Loss: 0.8435
2024-06-22 11:43:15,579 Epoch 1/8, Train Loss: 1.1761, Train Accuracy: 0.5180
2024-06-22 11:44:00,445 Epoch 1/8, Val Loss: 0.9351, Val Accuracy: 0.6000
2024-06-22 11:44:01,484 Epoch [2/8], Batch [1/374], Loss: 0.8922
2024-06-22 11:44:53,432 Epoch [2/8], Batch [51/374], Loss: 0.8457
2024-06-22 11:45:45,307 Epoch [2/8], Batch [101/374], Loss: 0.8055
2024-06-22 11:46:37,381 Epoch [2/8], Batch [151/374], Loss: 0.9450
2024-06-22 11:47:29,308 Epoch [2/8], Batch [201/374], Loss: 0.6963
2024-06-22 11:48:21,437 Epoch [2/8], Batch [251/374], Loss: 0.8196
2024-06-22 11:49:13,305 Epoch [2/8], Batch [301/374], Loss: 0.9105
2024-06-22 11:50:05,694 Epoch [2/8], Batch [351/374], Loss: 0.6403
2024-06-22 11:50:29,435 Epoch 2/8, Train Loss: 0.8742, Train Accuracy: 0.6221
2024-06-22 11:51:14,235 Epoch 2/8, Val Loss: 0.8597, Val Accuracy: 0.6361
2024-06-22 11:51:15,277 Epoch [3/8], Batch [1/374], Loss: 0.7245
2024-06-22 11:52:07,292 Epoch [3/8], Batch [51/374], Loss: 0.5951
2024-06-22 11:52:59,840 Epoch [3/8], Batch [101/374], Loss: 0.6126
2024-06-22 11:53:52,036 Epoch [3/8], Batch [151/374], Loss: 0.8418
2024-06-22 11:54:44,314 Epoch [3/8], Batch [201/374], Loss: 0.7816
2024-06-22 11:55:36,049 Epoch [3/8], Batch [251/374], Loss: 0.8032
2024-06-22 11:56:28,336 Epoch [3/8], Batch [301/374], Loss: 1.0428
2024-06-22 11:57:20,275 Epoch [3/8], Batch [351/374], Loss: 0.7846
2024-06-22 11:57:44,360 Epoch 3/8, Train Loss: 0.7505, Train Accuracy: 0.6736
2024-06-22 11:58:29,225 Epoch 3/8, Val Loss: 0.8993, Val Accuracy: 0.6177
2024-06-22 11:58:30,287 Epoch [4/8], Batch [1/374], Loss: 0.7049
2024-06-22 11:59:22,458 Epoch [4/8], Batch [51/374], Loss: 0.6451
2024-06-22 12:00:14,463 Epoch [4/8], Batch [101/374], Loss: 0.6204
2024-06-22 12:01:06,745 Epoch [4/8], Batch [151/374], Loss: 0.7245
2024-06-22 12:01:58,927 Epoch [4/8], Batch [201/374], Loss: 0.5641
2024-06-22 12:02:51,279 Epoch [4/8], Batch [251/374], Loss: 0.5479
2024-06-22 12:03:43,707 Epoch [4/8], Batch [301/374], Loss: 0.4389
2024-06-22 12:04:35,845 Epoch [4/8], Batch [351/374], Loss: 0.4797
2024-06-22 12:04:59,784 Epoch 4/8, Train Loss: 0.6572, Train Accuracy: 0.7131
2024-06-22 12:05:44,708 Epoch 4/8, Val Loss: 0.9305, Val Accuracy: 0.5970
2024-06-22 12:05:45,731 Epoch [5/8], Batch [1/374], Loss: 0.6557
2024-06-22 12:06:37,967 Epoch [5/8], Batch [51/374], Loss: 0.6842
2024-06-22 12:07:30,329 Epoch [5/8], Batch [101/374], Loss: 0.5674
2024-06-22 12:08:22,719 Epoch [5/8], Batch [151/374], Loss: 0.4759
2024-06-22 12:09:15,018 Epoch [5/8], Batch [201/374], Loss: 0.4655
2024-06-22 12:10:07,041 Epoch [5/8], Batch [251/374], Loss: 0.4193
2024-06-22 12:10:59,402 Epoch [5/8], Batch [301/374], Loss: 0.4849
2024-06-22 12:11:51,601 Epoch [5/8], Batch [351/374], Loss: 0.4956
2024-06-22 12:12:15,297 Epoch 5/8, Train Loss: 0.5405, Train Accuracy: 0.7628
2024-06-22 12:13:00,792 Epoch 5/8, Val Loss: 0.8330, Val Accuracy: 0.6498
2024-06-22 12:13:01,814 Epoch [6/8], Batch [1/374], Loss: 0.3874
2024-06-22 12:13:53,516 Epoch [6/8], Batch [51/374], Loss: 0.4586
2024-06-22 12:14:45,721 Epoch [6/8], Batch [101/374], Loss: 0.3188
2024-06-22 12:15:38,006 Epoch [6/8], Batch [151/374], Loss: 0.4229
2024-06-22 12:16:30,269 Epoch [6/8], Batch [201/374], Loss: 0.5482
2024-06-22 12:17:22,455 Epoch [6/8], Batch [251/374], Loss: 0.5506
2024-06-22 12:18:14,516 Epoch [6/8], Batch [301/374], Loss: 0.4110
2024-06-22 12:19:06,852 Epoch [6/8], Batch [351/374], Loss: 0.3274
2024-06-22 12:19:30,596 Epoch 6/8, Train Loss: 0.4464, Train Accuracy: 0.8106
2024-06-22 12:20:15,485 Epoch 6/8, Val Loss: 0.9409, Val Accuracy: 0.6515
2024-06-22 12:20:16,507 Epoch [7/8], Batch [1/374], Loss: 0.3548
2024-06-22 12:21:08,476 Epoch [7/8], Batch [51/374], Loss: 0.3137
2024-06-22 12:22:00,296 Epoch [7/8], Batch [101/374], Loss: 0.4260
2024-06-22 12:22:52,764 Epoch [7/8], Batch [151/374], Loss: 0.3267
2024-06-22 12:23:45,027 Epoch [7/8], Batch [201/374], Loss: 0.4308
2024-06-22 12:24:37,372 Epoch [7/8], Batch [251/374], Loss: 0.5196
2024-06-22 12:25:29,569 Epoch [7/8], Batch [301/374], Loss: 0.4535
2024-06-22 12:26:21,601 Epoch [7/8], Batch [351/374], Loss: 0.2469
2024-06-22 12:26:45,484 Epoch 7/8, Train Loss: 0.3766, Train Accuracy: 0.8361
2024-06-22 12:27:30,305 Epoch 7/8, Val Loss: 1.0086, Val Accuracy: 0.6522
2024-06-22 12:27:31,339 Epoch [8/8], Batch [1/374], Loss: 0.2292
2024-06-22 12:28:23,553 Epoch [8/8], Batch [51/374], Loss: 0.3342
2024-06-22 12:29:15,631 Epoch [8/8], Batch [101/374], Loss: 0.2853
2024-06-22 12:30:07,919 Epoch [8/8], Batch [151/374], Loss: 0.2836
2024-06-22 12:31:00,136 Epoch [8/8], Batch [201/374], Loss: 0.3352
2024-06-22 12:31:52,356 Epoch [8/8], Batch [251/374], Loss: 0.4212
2024-06-22 12:32:44,596 Epoch [8/8], Batch [301/374], Loss: 0.3517
2024-06-22 12:33:36,743 Epoch [8/8], Batch [351/374], Loss: 0.1928
2024-06-22 12:34:00,476 Epoch 8/8, Train Loss: 0.3161, Train Accuracy: 0.8685
2024-06-22 12:34:45,521 Epoch 8/8, Val Loss: 1.1313, Val Accuracy: 0.6411
2024-06-22 12:34:45,522 Training finished!
2024-06-22 12:34:45,522 ==================================================
2024-06-22 12:34:46,294 ==================================================
2024-06-22 12:34:46,294 Training with Guassian Noise of 128 length...
2024-06-22 12:34:47,356 Epoch [1/8], Batch [1/374], Loss: 1.5942
2024-06-22 12:35:39,153 Epoch [1/8], Batch [51/374], Loss: 1.5771
2024-06-22 12:36:31,299 Epoch [1/8], Batch [101/374], Loss: 1.3602
2024-06-22 12:37:23,519 Epoch [1/8], Batch [151/374], Loss: 1.3930
2024-06-22 12:38:15,473 Epoch [1/8], Batch [201/374], Loss: 1.0166
2024-06-22 12:39:07,443 Epoch [1/8], Batch [251/374], Loss: 1.0652
2024-06-22 12:39:59,576 Epoch [1/8], Batch [301/374], Loss: 1.0561
2024-06-22 12:40:51,771 Epoch [1/8], Batch [351/374], Loss: 0.7952
2024-06-22 12:41:15,576 Epoch 1/8, Train Loss: 1.2306, Train Accuracy: 0.4766
2024-06-22 12:42:00,343 Epoch 1/8, Val Loss: 1.0281, Val Accuracy: 0.5672
2024-06-22 12:42:01,380 Epoch [2/8], Batch [1/374], Loss: 1.1909
2024-06-22 12:42:53,557 Epoch [2/8], Batch [51/374], Loss: 0.6899
2024-06-22 12:43:45,604 Epoch [2/8], Batch [101/374], Loss: 0.9797
2024-06-22 12:44:38,274 Epoch [2/8], Batch [151/374], Loss: 1.1062
2024-06-22 12:45:30,551 Epoch [2/8], Batch [201/374], Loss: 0.9119
2024-06-22 12:46:23,264 Epoch [2/8], Batch [251/374], Loss: 1.0563
2024-06-22 12:47:16,310 Epoch [2/8], Batch [301/374], Loss: 1.1429
2024-06-22 12:48:09,202 Epoch [2/8], Batch [351/374], Loss: 0.8489
2024-06-22 12:48:33,345 Epoch 2/8, Train Loss: 0.8899, Train Accuracy: 0.6110
2024-06-22 12:49:19,644 Epoch 2/8, Val Loss: 0.9900, Val Accuracy: 0.5545
2024-06-22 12:49:20,694 Epoch [3/8], Batch [1/374], Loss: 0.8950
2024-06-22 12:50:13,547 Epoch [3/8], Batch [51/374], Loss: 0.5232
2024-06-22 12:51:06,480 Epoch [3/8], Batch [101/374], Loss: 0.7760
2024-06-22 12:51:59,312 Epoch [3/8], Batch [151/374], Loss: 0.8093
2024-06-22 12:52:52,043 Epoch [3/8], Batch [201/374], Loss: 0.8522
2024-06-22 12:53:44,992 Epoch [3/8], Batch [251/374], Loss: 0.8230
2024-06-22 12:54:37,830 Epoch [3/8], Batch [301/374], Loss: 0.8290
2024-06-22 12:55:30,445 Epoch [3/8], Batch [351/374], Loss: 0.6853
2024-06-22 12:55:54,270 Epoch 3/8, Train Loss: 0.7648, Train Accuracy: 0.6560
2024-06-22 12:56:39,877 Epoch 3/8, Val Loss: 0.9935, Val Accuracy: 0.5672
2024-06-22 12:56:40,921 Epoch [4/8], Batch [1/374], Loss: 0.5628
2024-06-22 12:57:33,019 Epoch [4/8], Batch [51/374], Loss: 0.6226
2024-06-22 12:58:25,336 Epoch [4/8], Batch [101/374], Loss: 0.7411
2024-06-22 12:59:18,110 Epoch [4/8], Batch [151/374], Loss: 0.6866
2024-06-22 13:00:10,667 Epoch [4/8], Batch [201/374], Loss: 0.6927
2024-06-22 13:01:03,355 Epoch [4/8], Batch [251/374], Loss: 0.6613
2024-06-22 13:01:56,775 Epoch [4/8], Batch [301/374], Loss: 0.5838
2024-06-22 13:02:50,033 Epoch [4/8], Batch [351/374], Loss: 0.6800
2024-06-22 13:03:14,078 Epoch 4/8, Train Loss: 0.6671, Train Accuracy: 0.7067
2024-06-22 13:04:00,335 Epoch 4/8, Val Loss: 0.9046, Val Accuracy: 0.6247
2024-06-22 13:04:01,370 Epoch [5/8], Batch [1/374], Loss: 0.6272
2024-06-22 13:04:54,201 Epoch [5/8], Batch [51/374], Loss: 0.5208
2024-06-22 13:05:46,663 Epoch [5/8], Batch [101/374], Loss: 0.6238
2024-06-22 13:06:39,561 Epoch [5/8], Batch [151/374], Loss: 0.3126
2024-06-22 13:07:32,325 Epoch [5/8], Batch [201/374], Loss: 0.9861
2024-06-22 13:08:25,255 Epoch [5/8], Batch [251/374], Loss: 0.7015
2024-06-22 13:09:18,326 Epoch [5/8], Batch [301/374], Loss: 0.8278
2024-06-22 13:10:11,277 Epoch [5/8], Batch [351/374], Loss: 0.5132
2024-06-22 13:10:35,632 Epoch 5/8, Train Loss: 0.5741, Train Accuracy: 0.7545
2024-06-22 13:11:22,441 Epoch 5/8, Val Loss: 1.0215, Val Accuracy: 0.5903
2024-06-22 13:11:23,496 Epoch [6/8], Batch [1/374], Loss: 0.3443
2024-06-22 13:12:16,307 Epoch [6/8], Batch [51/374], Loss: 0.6422
2024-06-22 13:13:09,427 Epoch [6/8], Batch [101/374], Loss: 0.5309
2024-06-22 13:14:02,262 Epoch [6/8], Batch [151/374], Loss: 0.5214
2024-06-22 13:14:55,095 Epoch [6/8], Batch [201/374], Loss: 0.4825
2024-06-22 13:15:48,348 Epoch [6/8], Batch [251/374], Loss: 0.5305
2024-06-22 13:16:41,410 Epoch [6/8], Batch [301/374], Loss: 0.4537
2024-06-22 13:17:34,165 Epoch [6/8], Batch [351/374], Loss: 0.4501
2024-06-22 13:17:58,409 Epoch 6/8, Train Loss: 0.4838, Train Accuracy: 0.7994
2024-06-22 13:18:45,357 Epoch 6/8, Val Loss: 0.9823, Val Accuracy: 0.6201
2024-06-22 13:18:46,420 Epoch [7/8], Batch [1/374], Loss: 0.4831
2024-06-22 13:19:39,559 Epoch [7/8], Batch [51/374], Loss: 0.2892
2024-06-22 13:20:32,528 Epoch [7/8], Batch [101/374], Loss: 0.4159
2024-06-22 13:21:25,221 Epoch [7/8], Batch [151/374], Loss: 0.4848
2024-06-22 13:22:18,298 Epoch [7/8], Batch [201/374], Loss: 0.4265
2024-06-22 13:23:11,222 Epoch [7/8], Batch [251/374], Loss: 0.4058
2024-06-22 13:24:03,587 Epoch [7/8], Batch [301/374], Loss: 0.4199
2024-06-22 13:24:56,027 Epoch [7/8], Batch [351/374], Loss: 0.4452
2024-06-22 13:25:20,135 Epoch 7/8, Train Loss: 0.3958, Train Accuracy: 0.8399
2024-06-22 13:26:05,937 Epoch 7/8, Val Loss: 0.9414, Val Accuracy: 0.6602
2024-06-22 13:26:07,031 Epoch [8/8], Batch [1/374], Loss: 0.3179
2024-06-22 13:26:59,375 Epoch [8/8], Batch [51/374], Loss: 0.3753
2024-06-22 13:27:51,919 Epoch [8/8], Batch [101/374], Loss: 0.3083
2024-06-22 13:28:44,710 Epoch [8/8], Batch [151/374], Loss: 0.2381
2024-06-22 13:29:37,737 Epoch [8/8], Batch [201/374], Loss: 0.3095
2024-06-22 13:30:30,396 Epoch [8/8], Batch [251/374], Loss: 0.2204
2024-06-22 13:31:23,261 Epoch [8/8], Batch [301/374], Loss: 0.2422
2024-06-22 13:32:15,675 Epoch [8/8], Batch [351/374], Loss: 0.1689
2024-06-22 13:32:40,039 Epoch 8/8, Train Loss: 0.3021, Train Accuracy: 0.8873
2024-06-22 13:33:26,401 Epoch 8/8, Val Loss: 1.2025, Val Accuracy: 0.6224
2024-06-22 13:33:26,405 Training finished!
2024-06-22 13:33:26,405 ==================================================

