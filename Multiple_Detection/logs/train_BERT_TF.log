2024-06-22 16:49:41,088 ==================================================
2024-06-22 16:49:41,089 Training BERT + TF-IDF features with LR-1e-06...
2024-06-22 16:49:42,938 Epoch [1/8], Batch [1/748], Loss: 1.6090
2024-06-22 16:50:32,061 Epoch [1/8], Batch [51/748], Loss: 1.6130
2024-06-22 16:51:22,415 Epoch [1/8], Batch [101/748], Loss: 1.6030
2024-06-22 16:52:12,752 Epoch [1/8], Batch [151/748], Loss: 1.5892
2024-06-22 16:53:03,706 Epoch [1/8], Batch [201/748], Loss: 1.5796
2024-06-22 16:53:54,476 Epoch [1/8], Batch [251/748], Loss: 1.5578
2024-06-22 16:54:45,435 Epoch [1/8], Batch [301/748], Loss: 1.5212
2024-06-22 16:55:35,969 Epoch [1/8], Batch [351/748], Loss: 1.4808
2024-06-22 16:56:26,454 Epoch [1/8], Batch [401/748], Loss: 1.4929
2024-06-22 16:57:17,175 Epoch [1/8], Batch [451/748], Loss: 1.5001
2024-06-22 16:58:08,253 Epoch [1/8], Batch [501/748], Loss: 1.3398
2024-06-22 16:58:59,131 Epoch [1/8], Batch [551/748], Loss: 1.4186
2024-06-22 16:59:50,089 Epoch [1/8], Batch [601/748], Loss: 1.3788
2024-06-22 17:00:41,073 Epoch [1/8], Batch [651/748], Loss: 1.4204
2024-06-22 17:01:31,976 Epoch [1/8], Batch [701/748], Loss: 1.3104
2024-06-22 17:02:19,463 Epoch 1/8, Train Loss: 1.4846, Train Accuracy: 0.3653
2024-06-22 17:03:44,410 Epoch 1/8, Val Loss: 1.3273, Val Accuracy: 0.4509
2024-06-22 17:03:45,456 Epoch [2/8], Batch [1/748], Loss: 1.4802
2024-06-22 17:04:36,418 Epoch [2/8], Batch [51/748], Loss: 1.2795
2024-06-22 17:05:26,994 Epoch [2/8], Batch [101/748], Loss: 1.2084
2024-06-22 17:06:17,525 Epoch [2/8], Batch [151/748], Loss: 1.3353
2024-06-22 17:07:08,154 Epoch [2/8], Batch [201/748], Loss: 1.2915
2024-06-22 17:07:59,228 Epoch [2/8], Batch [251/748], Loss: 1.1485
2024-06-22 17:08:50,016 Epoch [2/8], Batch [301/748], Loss: 1.1850
2024-06-22 17:09:40,724 Epoch [2/8], Batch [351/748], Loss: 1.1828
2024-06-22 17:10:31,246 Epoch [2/8], Batch [401/748], Loss: 1.2899
2024-06-22 17:11:21,954 Epoch [2/8], Batch [451/748], Loss: 1.0943
2024-06-22 17:12:12,551 Epoch [2/8], Batch [501/748], Loss: 1.3867
2024-06-22 17:13:03,299 Epoch [2/8], Batch [551/748], Loss: 1.2220
2024-06-22 17:13:53,873 Epoch [2/8], Batch [601/748], Loss: 1.2140
2024-06-22 17:14:44,717 Epoch [2/8], Batch [651/748], Loss: 1.2448
2024-06-22 17:15:35,273 Epoch [2/8], Batch [701/748], Loss: 1.1348
2024-06-22 17:16:22,483 Epoch 2/8, Train Loss: 1.2338, Train Accuracy: 0.5036
2024-06-22 17:17:47,463 Epoch 2/8, Val Loss: 1.1426, Val Accuracy: 0.5313
2024-06-22 17:17:48,500 Epoch [3/8], Batch [1/748], Loss: 1.2162
2024-06-22 17:18:39,009 Epoch [3/8], Batch [51/748], Loss: 1.1556
2024-06-22 17:19:29,939 Epoch [3/8], Batch [101/748], Loss: 1.2097
2024-06-22 17:20:20,443 Epoch [3/8], Batch [151/748], Loss: 1.1136
2024-06-22 17:21:11,205 Epoch [3/8], Batch [201/748], Loss: 1.2650
2024-06-22 17:22:01,691 Epoch [3/8], Batch [251/748], Loss: 1.1141
2024-06-22 17:22:52,506 Epoch [3/8], Batch [301/748], Loss: 1.0135
2024-06-22 17:23:42,896 Epoch [3/8], Batch [351/748], Loss: 1.2555
2024-06-22 17:24:33,640 Epoch [3/8], Batch [401/748], Loss: 1.0616
2024-06-22 17:25:24,337 Epoch [3/8], Batch [451/748], Loss: 1.1143
2024-06-22 17:26:14,870 Epoch [3/8], Batch [501/748], Loss: 0.9434
2024-06-22 17:27:05,503 Epoch [3/8], Batch [551/748], Loss: 1.2565
2024-06-22 17:27:56,340 Epoch [3/8], Batch [601/748], Loss: 1.1406
2024-06-22 17:28:47,351 Epoch [3/8], Batch [651/748], Loss: 1.0955
2024-06-22 17:29:38,146 Epoch [3/8], Batch [701/748], Loss: 1.0203
2024-06-22 17:30:25,365 Epoch 3/8, Train Loss: 1.0863, Train Accuracy: 0.5332
2024-06-22 17:31:50,070 Epoch 3/8, Val Loss: 1.0580, Val Accuracy: 0.5419
2024-06-22 17:31:51,061 Epoch [4/8], Batch [1/748], Loss: 1.0357
2024-06-22 17:32:41,780 Epoch [4/8], Batch [51/748], Loss: 1.0603
2024-06-22 17:33:32,585 Epoch [4/8], Batch [101/748], Loss: 0.9576
2024-06-22 17:34:23,394 Epoch [4/8], Batch [151/748], Loss: 1.0168
2024-06-22 17:35:13,945 Epoch [4/8], Batch [201/748], Loss: 1.0482
2024-06-22 17:36:04,686 Epoch [4/8], Batch [251/748], Loss: 1.0079
2024-06-22 17:36:55,406 Epoch [4/8], Batch [301/748], Loss: 1.0075
2024-06-22 17:37:45,974 Epoch [4/8], Batch [351/748], Loss: 1.1691
2024-06-22 17:38:36,514 Epoch [4/8], Batch [401/748], Loss: 0.9452
2024-06-22 17:39:27,132 Epoch [4/8], Batch [451/748], Loss: 0.8353
2024-06-22 17:40:17,598 Epoch [4/8], Batch [501/748], Loss: 1.0462
2024-06-22 17:41:08,072 Epoch [4/8], Batch [551/748], Loss: 0.9087
2024-06-22 17:41:58,596 Epoch [4/8], Batch [601/748], Loss: 0.7807
2024-06-22 17:42:49,160 Epoch [4/8], Batch [651/748], Loss: 0.8300
2024-06-22 17:43:40,287 Epoch [4/8], Batch [701/748], Loss: 0.8874
2024-06-22 17:44:28,578 Epoch 4/8, Train Loss: 1.0003, Train Accuracy: 0.5490
2024-06-22 17:45:54,765 Epoch 4/8, Val Loss: 0.9897, Val Accuracy: 0.5579
2024-06-22 17:45:55,788 Epoch [5/8], Batch [1/748], Loss: 0.9124
2024-06-22 17:46:46,783 Epoch [5/8], Batch [51/748], Loss: 1.0083
2024-06-22 17:47:37,835 Epoch [5/8], Batch [101/748], Loss: 0.9714
2024-06-22 17:48:28,700 Epoch [5/8], Batch [151/748], Loss: 1.0787
2024-06-22 17:49:20,485 Epoch [5/8], Batch [201/748], Loss: 0.8039
2024-06-22 17:50:11,130 Epoch [5/8], Batch [251/748], Loss: 0.7394
2024-06-22 17:51:02,765 Epoch [5/8], Batch [301/748], Loss: 1.0166
2024-06-22 17:51:54,368 Epoch [5/8], Batch [351/748], Loss: 1.1349
2024-06-22 17:52:45,748 Epoch [5/8], Batch [401/748], Loss: 0.8596
2024-06-22 17:53:36,805 Epoch [5/8], Batch [451/748], Loss: 0.9825
2024-06-22 17:54:28,118 Epoch [5/8], Batch [501/748], Loss: 0.8612
2024-06-22 17:55:19,894 Epoch [5/8], Batch [551/748], Loss: 1.0127
2024-06-22 17:56:11,490 Epoch [5/8], Batch [601/748], Loss: 1.1225
2024-06-22 17:57:02,782 Epoch [5/8], Batch [651/748], Loss: 0.9515
2024-06-22 17:57:54,741 Epoch [5/8], Batch [701/748], Loss: 0.7684
2024-06-22 17:58:42,674 Epoch 5/8, Train Loss: 0.9509, Train Accuracy: 0.5574
2024-06-22 18:00:10,159 Epoch 5/8, Val Loss: 0.9702, Val Accuracy: 0.5544
2024-06-22 18:00:11,191 Epoch [6/8], Batch [1/748], Loss: 0.8520
2024-06-22 18:01:02,061 Epoch [6/8], Batch [51/748], Loss: 0.9182
2024-06-22 18:01:52,604 Epoch [6/8], Batch [101/748], Loss: 0.8347
2024-06-22 18:02:43,571 Epoch [6/8], Batch [151/748], Loss: 0.6986
2024-06-22 18:03:34,402 Epoch [6/8], Batch [201/748], Loss: 0.8804
2024-06-22 18:04:25,214 Epoch [6/8], Batch [251/748], Loss: 0.9151
2024-06-22 18:05:16,365 Epoch [6/8], Batch [301/748], Loss: 0.9525
2024-06-22 18:06:07,140 Epoch [6/8], Batch [351/748], Loss: 1.0642
2024-06-22 18:06:58,277 Epoch [6/8], Batch [401/748], Loss: 0.9867
2024-06-22 18:07:49,073 Epoch [6/8], Batch [451/748], Loss: 1.0828
2024-06-22 18:08:39,927 Epoch [6/8], Batch [501/748], Loss: 0.8069
2024-06-22 18:09:30,653 Epoch [6/8], Batch [551/748], Loss: 0.8678
2024-06-22 18:10:21,123 Epoch [6/8], Batch [601/748], Loss: 0.9473
2024-06-22 18:11:11,827 Epoch [6/8], Batch [651/748], Loss: 1.0025
2024-06-22 18:12:02,673 Epoch [6/8], Batch [701/748], Loss: 0.7195
2024-06-22 18:12:50,084 Epoch 6/8, Train Loss: 0.9146, Train Accuracy: 0.5730
2024-06-22 18:14:14,928 Epoch 6/8, Val Loss: 0.9487, Val Accuracy: 0.5798
2024-06-22 18:14:15,907 Epoch [7/8], Batch [1/748], Loss: 1.0828
2024-06-22 18:15:06,229 Epoch [7/8], Batch [51/748], Loss: 0.9720
2024-06-22 18:15:57,005 Epoch [7/8], Batch [101/748], Loss: 1.1024
2024-06-22 18:16:48,016 Epoch [7/8], Batch [151/748], Loss: 0.9110
2024-06-22 18:17:38,978 Epoch [7/8], Batch [201/748], Loss: 0.7174
2024-06-22 18:18:29,963 Epoch [7/8], Batch [251/748], Loss: 0.9180
2024-06-22 18:19:20,809 Epoch [7/8], Batch [301/748], Loss: 1.0797
2024-06-22 18:20:11,826 Epoch [7/8], Batch [351/748], Loss: 1.0273
2024-06-22 18:21:02,712 Epoch [7/8], Batch [401/748], Loss: 0.8432
2024-06-22 18:21:53,562 Epoch [7/8], Batch [451/748], Loss: 0.7229
2024-06-22 18:22:44,757 Epoch [7/8], Batch [501/748], Loss: 0.9760
2024-06-22 18:23:35,585 Epoch [7/8], Batch [551/748], Loss: 0.8543
2024-06-22 18:24:26,423 Epoch [7/8], Batch [601/748], Loss: 0.7419
2024-06-22 18:25:17,338 Epoch [7/8], Batch [651/748], Loss: 0.9265
2024-06-22 18:26:08,302 Epoch [7/8], Batch [701/748], Loss: 0.7317
2024-06-22 18:26:55,795 Epoch 7/8, Train Loss: 0.8902, Train Accuracy: 0.5920
2024-06-22 18:28:20,874 Epoch 7/8, Val Loss: 0.9322, Val Accuracy: 0.5760
2024-06-22 18:28:21,915 Epoch [8/8], Batch [1/748], Loss: 0.9824
2024-06-22 18:29:12,704 Epoch [8/8], Batch [51/748], Loss: 0.7078
2024-06-22 18:30:03,635 Epoch [8/8], Batch [101/748], Loss: 0.8567
2024-06-22 18:30:54,660 Epoch [8/8], Batch [151/748], Loss: 0.8301
2024-06-22 18:31:45,549 Epoch [8/8], Batch [201/748], Loss: 0.8875
2024-06-22 18:32:36,657 Epoch [8/8], Batch [251/748], Loss: 0.7530
2024-06-22 18:33:27,470 Epoch [8/8], Batch [301/748], Loss: 0.9361
2024-06-22 18:34:18,382 Epoch [8/8], Batch [351/748], Loss: 0.9519
2024-06-22 18:35:09,202 Epoch [8/8], Batch [401/748], Loss: 0.9468
2024-06-22 18:36:00,281 Epoch [8/8], Batch [451/748], Loss: 0.9176
2024-06-22 18:36:51,222 Epoch [8/8], Batch [501/748], Loss: 0.7430
2024-06-22 18:37:42,168 Epoch [8/8], Batch [551/748], Loss: 1.0405
2024-06-22 18:38:33,168 Epoch [8/8], Batch [601/748], Loss: 0.6477
2024-06-22 18:39:24,126 Epoch [8/8], Batch [651/748], Loss: 0.8876
2024-06-22 18:40:15,044 Epoch [8/8], Batch [701/748], Loss: 0.9675
2024-06-22 18:41:02,596 Epoch 8/8, Train Loss: 0.8692, Train Accuracy: 0.5951
2024-06-22 18:42:27,708 Epoch 8/8, Val Loss: 0.9455, Val Accuracy: 0.5645
2024-06-22 18:42:27,711 Training finished!
2024-06-22 18:42:27,711 ==================================================
2024-06-22 18:42:35,210 ==================================================
2024-06-22 18:42:35,210 Training BERT + TF-IDF features with LR-6e-06...
2024-06-22 18:42:36,225 Epoch [1/8], Batch [1/748], Loss: 1.6037
2024-06-22 18:43:26,566 Epoch [1/8], Batch [51/748], Loss: 1.5812
2024-06-22 18:44:17,689 Epoch [1/8], Batch [101/748], Loss: 1.4832
2024-06-22 18:45:08,476 Epoch [1/8], Batch [151/748], Loss: 1.3516
2024-06-22 18:45:59,417 Epoch [1/8], Batch [201/748], Loss: 1.1795
2024-06-22 18:46:50,498 Epoch [1/8], Batch [251/748], Loss: 1.1625
2024-06-22 18:47:41,257 Epoch [1/8], Batch [301/748], Loss: 1.0235
2024-06-22 18:48:32,330 Epoch [1/8], Batch [351/748], Loss: 1.1464
2024-06-22 18:49:23,109 Epoch [1/8], Batch [401/748], Loss: 1.1083
2024-06-22 18:50:14,172 Epoch [1/8], Batch [451/748], Loss: 0.9973
2024-06-22 18:51:05,035 Epoch [1/8], Batch [501/748], Loss: 0.9994
2024-06-22 18:51:56,029 Epoch [1/8], Batch [551/748], Loss: 1.0448
2024-06-22 18:52:46,890 Epoch [1/8], Batch [601/748], Loss: 0.9177
2024-06-22 18:53:37,707 Epoch [1/8], Batch [651/748], Loss: 0.9358
2024-06-22 18:54:28,795 Epoch [1/8], Batch [701/748], Loss: 0.8942
2024-06-22 18:55:16,279 Epoch 1/8, Train Loss: 1.1611, Train Accuracy: 0.5134
2024-06-22 18:56:40,872 Epoch 1/8, Val Loss: 0.9362, Val Accuracy: 0.5847
2024-06-22 18:56:41,876 Epoch [2/8], Batch [1/748], Loss: 0.8134
2024-06-22 18:57:32,171 Epoch [2/8], Batch [51/748], Loss: 0.7266
2024-06-22 18:58:23,111 Epoch [2/8], Batch [101/748], Loss: 0.9524
2024-06-22 18:59:13,969 Epoch [2/8], Batch [151/748], Loss: 0.9768
2024-06-22 19:00:04,984 Epoch [2/8], Batch [201/748], Loss: 0.6968
2024-06-22 19:00:55,887 Epoch [2/8], Batch [251/748], Loss: 0.6722
2024-06-22 19:01:46,449 Epoch [2/8], Batch [301/748], Loss: 0.7488
2024-06-22 19:02:36,984 Epoch [2/8], Batch [351/748], Loss: 0.8136
2024-06-22 19:03:27,745 Epoch [2/8], Batch [401/748], Loss: 0.7772
2024-06-22 19:04:18,426 Epoch [2/8], Batch [451/748], Loss: 0.8290
2024-06-22 19:05:09,005 Epoch [2/8], Batch [501/748], Loss: 0.8884
2024-06-22 19:05:59,732 Epoch [2/8], Batch [551/748], Loss: 0.7444
2024-06-22 19:06:50,694 Epoch [2/8], Batch [601/748], Loss: 0.9912
2024-06-22 19:07:41,352 Epoch [2/8], Batch [651/748], Loss: 0.8608
2024-06-22 19:08:32,099 Epoch [2/8], Batch [701/748], Loss: 0.6699
2024-06-22 19:09:19,052 Epoch 2/8, Train Loss: 0.8735, Train Accuracy: 0.6158
2024-06-22 19:10:43,342 Epoch 2/8, Val Loss: 0.9504, Val Accuracy: 0.5967
2024-06-22 19:10:44,351 Epoch [3/8], Batch [1/748], Loss: 0.7517
2024-06-22 19:11:34,570 Epoch [3/8], Batch [51/748], Loss: 0.9645
2024-06-22 19:12:24,965 Epoch [3/8], Batch [101/748], Loss: 0.6838
2024-06-22 19:13:15,970 Epoch [3/8], Batch [151/748], Loss: 0.9905
2024-06-22 19:14:06,599 Epoch [3/8], Batch [201/748], Loss: 0.9505
2024-06-22 19:14:57,198 Epoch [3/8], Batch [251/748], Loss: 0.6647
2024-06-22 19:15:47,781 Epoch [3/8], Batch [301/748], Loss: 0.5357
2024-06-22 19:16:38,733 Epoch [3/8], Batch [351/748], Loss: 0.8393
2024-06-22 19:17:29,570 Epoch [3/8], Batch [401/748], Loss: 0.8624
2024-06-22 19:18:20,237 Epoch [3/8], Batch [451/748], Loss: 0.7832
2024-06-22 19:19:10,882 Epoch [3/8], Batch [501/748], Loss: 0.7360
2024-06-22 19:20:01,560 Epoch [3/8], Batch [551/748], Loss: 0.7665
2024-06-22 19:20:52,389 Epoch [3/8], Batch [601/748], Loss: 0.7701
2024-06-22 19:21:43,011 Epoch [3/8], Batch [651/748], Loss: 0.7175
2024-06-22 19:22:33,582 Epoch [3/8], Batch [701/748], Loss: 0.7379
2024-06-22 19:23:20,817 Epoch 3/8, Train Loss: 0.7778, Train Accuracy: 0.6502
2024-06-22 19:24:44,832 Epoch 3/8, Val Loss: 0.8386, Val Accuracy: 0.6348
2024-06-22 19:24:45,835 Epoch [4/8], Batch [1/748], Loss: 0.6259
2024-06-22 19:25:36,403 Epoch [4/8], Batch [51/748], Loss: 0.5117
2024-06-22 19:26:27,028 Epoch [4/8], Batch [101/748], Loss: 0.7920
2024-06-22 19:27:17,690 Epoch [4/8], Batch [151/748], Loss: 0.9074
2024-06-22 19:28:08,324 Epoch [4/8], Batch [201/748], Loss: 0.6318
2024-06-22 19:28:59,023 Epoch [4/8], Batch [251/748], Loss: 0.8979
2024-06-22 19:29:49,508 Epoch [4/8], Batch [301/748], Loss: 0.8039
2024-06-22 19:30:40,405 Epoch [4/8], Batch [351/748], Loss: 0.4168
2024-06-22 19:31:30,929 Epoch [4/8], Batch [401/748], Loss: 0.7579
2024-06-22 19:32:21,365 Epoch [4/8], Batch [451/748], Loss: 0.6495
2024-06-22 19:33:12,243 Epoch [4/8], Batch [501/748], Loss: 0.7487
2024-06-22 19:34:02,685 Epoch [4/8], Batch [551/748], Loss: 0.4786
2024-06-22 19:34:53,366 Epoch [4/8], Batch [601/748], Loss: 0.6448
2024-06-22 19:35:43,812 Epoch [4/8], Batch [651/748], Loss: 0.8206
2024-06-22 19:36:34,327 Epoch [4/8], Batch [701/748], Loss: 0.7924
2024-06-22 19:37:21,351 Epoch 4/8, Train Loss: 0.7113, Train Accuracy: 0.6802
2024-06-22 19:38:45,342 Epoch 4/8, Val Loss: 0.8790, Val Accuracy: 0.6243
2024-06-22 19:38:46,336 Epoch [5/8], Batch [1/748], Loss: 0.8103
2024-06-22 19:39:36,667 Epoch [5/8], Batch [51/748], Loss: 0.6886
2024-06-22 19:40:27,129 Epoch [5/8], Batch [101/748], Loss: 0.7394
2024-06-22 19:41:17,652 Epoch [5/8], Batch [151/748], Loss: 0.6905
2024-06-22 19:42:08,472 Epoch [5/8], Batch [201/748], Loss: 0.6432
2024-06-22 19:42:59,473 Epoch [5/8], Batch [251/748], Loss: 0.7063
2024-06-22 19:43:49,872 Epoch [5/8], Batch [301/748], Loss: 0.7862
2024-06-22 19:44:40,536 Epoch [5/8], Batch [351/748], Loss: 0.5573
2024-06-22 19:45:31,336 Epoch [5/8], Batch [401/748], Loss: 0.6874
2024-06-22 19:46:21,892 Epoch [5/8], Batch [451/748], Loss: 0.6464
2024-06-22 19:47:12,425 Epoch [5/8], Batch [501/748], Loss: 0.8330
2024-06-22 19:48:02,930 Epoch [5/8], Batch [551/748], Loss: 0.8738
2024-06-22 19:48:53,293 Epoch [5/8], Batch [601/748], Loss: 0.6555
2024-06-22 19:49:44,029 Epoch [5/8], Batch [651/748], Loss: 0.5815
2024-06-22 19:50:34,573 Epoch [5/8], Batch [701/748], Loss: 0.6991
2024-06-22 19:51:21,620 Epoch 5/8, Train Loss: 0.6483, Train Accuracy: 0.7058
2024-06-22 19:52:45,797 Epoch 5/8, Val Loss: 0.8750, Val Accuracy: 0.6253
2024-06-22 19:52:46,818 Epoch [6/8], Batch [1/748], Loss: 0.5237
2024-06-22 19:53:37,212 Epoch [6/8], Batch [51/748], Loss: 0.5208
2024-06-22 19:54:27,656 Epoch [6/8], Batch [101/748], Loss: 0.7244
2024-06-22 19:55:18,471 Epoch [6/8], Batch [151/748], Loss: 0.6347
2024-06-22 19:56:08,912 Epoch [6/8], Batch [201/748], Loss: 0.7053
2024-06-22 19:56:59,613 Epoch [6/8], Batch [251/748], Loss: 0.6018
2024-06-22 19:57:50,242 Epoch [6/8], Batch [301/748], Loss: 0.6541
2024-06-22 19:58:40,805 Epoch [6/8], Batch [351/748], Loss: 0.5474
2024-06-22 19:59:31,231 Epoch [6/8], Batch [401/748], Loss: 0.5703
2024-06-22 20:00:22,055 Epoch [6/8], Batch [451/748], Loss: 0.6160
2024-06-22 20:01:13,000 Epoch [6/8], Batch [501/748], Loss: 0.3214
2024-06-22 20:02:03,497 Epoch [6/8], Batch [551/748], Loss: 0.9031
2024-06-22 20:02:54,072 Epoch [6/8], Batch [601/748], Loss: 0.5538
2024-06-22 20:03:44,527 Epoch [6/8], Batch [651/748], Loss: 0.5859
2024-06-22 20:04:35,042 Epoch [6/8], Batch [701/748], Loss: 0.5699
2024-06-22 20:05:22,488 Epoch 6/8, Train Loss: 0.5849, Train Accuracy: 0.7402
2024-06-22 20:06:47,102 Epoch 6/8, Val Loss: 0.9664, Val Accuracy: 0.6056
2024-06-22 20:06:48,136 Epoch [7/8], Batch [1/748], Loss: 0.4434
2024-06-22 20:07:38,754 Epoch [7/8], Batch [51/748], Loss: 0.4071
2024-06-22 20:08:29,649 Epoch [7/8], Batch [101/748], Loss: 0.5361
2024-06-22 20:09:20,293 Epoch [7/8], Batch [151/748], Loss: 0.6322
2024-06-22 20:10:11,182 Epoch [7/8], Batch [201/748], Loss: 0.4545
2024-06-22 20:11:02,177 Epoch [7/8], Batch [251/748], Loss: 0.4502
2024-06-22 20:11:53,022 Epoch [7/8], Batch [301/748], Loss: 0.6690
2024-06-22 20:12:43,987 Epoch [7/8], Batch [351/748], Loss: 0.3748
2024-06-22 20:13:34,957 Epoch [7/8], Batch [401/748], Loss: 0.4508
2024-06-22 20:14:25,868 Epoch [7/8], Batch [451/748], Loss: 0.5501
2024-06-22 20:15:16,721 Epoch [7/8], Batch [501/748], Loss: 0.4877
2024-06-22 20:16:07,743 Epoch [7/8], Batch [551/748], Loss: 0.3793
2024-06-22 20:16:58,726 Epoch [7/8], Batch [601/748], Loss: 0.5746
2024-06-22 20:17:49,527 Epoch [7/8], Batch [651/748], Loss: 0.2593
2024-06-22 20:18:40,517 Epoch [7/8], Batch [701/748], Loss: 0.4184
2024-06-22 20:19:27,965 Epoch 7/8, Train Loss: 0.5253, Train Accuracy: 0.7724
2024-06-22 20:20:52,647 Epoch 7/8, Val Loss: 0.9668, Val Accuracy: 0.6265
2024-06-22 20:20:53,666 Epoch [8/8], Batch [1/748], Loss: 0.5092
2024-06-22 20:21:43,966 Epoch [8/8], Batch [51/748], Loss: 0.5952
2024-06-22 20:22:34,888 Epoch [8/8], Batch [101/748], Loss: 0.4770
2024-06-22 20:23:25,878 Epoch [8/8], Batch [151/748], Loss: 0.4786
2024-06-22 20:24:16,638 Epoch [8/8], Batch [201/748], Loss: 0.3638
2024-06-22 20:25:07,563 Epoch [8/8], Batch [251/748], Loss: 0.3324
2024-06-22 20:25:58,292 Epoch [8/8], Batch [301/748], Loss: 0.4606
2024-06-22 20:26:49,471 Epoch [8/8], Batch [351/748], Loss: 0.6337
2024-06-22 20:27:40,424 Epoch [8/8], Batch [401/748], Loss: 0.3829
2024-06-22 20:28:31,299 Epoch [8/8], Batch [451/748], Loss: 0.2679
2024-06-22 20:29:22,118 Epoch [8/8], Batch [501/748], Loss: 0.5541
2024-06-22 20:30:13,089 Epoch [8/8], Batch [551/748], Loss: 0.3809
2024-06-22 20:31:04,032 Epoch [8/8], Batch [601/748], Loss: 0.7384
2024-06-22 20:31:54,822 Epoch [8/8], Batch [651/748], Loss: 0.3543
2024-06-22 20:32:45,767 Epoch [8/8], Batch [701/748], Loss: 0.3989
2024-06-22 20:33:33,278 Epoch 8/8, Train Loss: 0.4682, Train Accuracy: 0.7987
2024-06-22 20:34:58,126 Epoch 8/8, Val Loss: 0.9502, Val Accuracy: 0.6496
2024-06-22 20:34:58,127 Training finished!
2024-06-22 20:34:58,127 ==================================================
2024-06-22 20:35:04,835 ==================================================
2024-06-22 20:35:04,836 Training BERT + TF-IDF features with LR-1e-05...
2024-06-22 20:35:05,837 Epoch [1/8], Batch [1/748], Loss: 1.6138
2024-06-22 20:35:56,259 Epoch [1/8], Batch [51/748], Loss: 1.4567
2024-06-22 20:36:47,229 Epoch [1/8], Batch [101/748], Loss: 1.2339
2024-06-22 20:37:38,194 Epoch [1/8], Batch [151/748], Loss: 1.2981
2024-06-22 20:38:29,277 Epoch [1/8], Batch [201/748], Loss: 1.1606
2024-06-22 20:39:19,992 Epoch [1/8], Batch [251/748], Loss: 1.1254
2024-06-22 20:40:10,931 Epoch [1/8], Batch [301/748], Loss: 1.3729
2024-06-22 20:41:01,943 Epoch [1/8], Batch [351/748], Loss: 1.0331
2024-06-22 20:41:52,731 Epoch [1/8], Batch [401/748], Loss: 0.9825
2024-06-22 20:42:43,851 Epoch [1/8], Batch [451/748], Loss: 1.1648
2024-06-22 20:43:34,609 Epoch [1/8], Batch [501/748], Loss: 0.9468
2024-06-22 20:44:25,444 Epoch [1/8], Batch [551/748], Loss: 1.1271
2024-06-22 20:45:16,368 Epoch [1/8], Batch [601/748], Loss: 1.0041
2024-06-22 20:46:07,317 Epoch [1/8], Batch [651/748], Loss: 0.8201
2024-06-22 20:46:58,536 Epoch [1/8], Batch [701/748], Loss: 0.9856
2024-06-22 20:47:45,885 Epoch 1/8, Train Loss: 1.1125, Train Accuracy: 0.5191
2024-06-22 20:49:11,577 Epoch 1/8, Val Loss: 0.9281, Val Accuracy: 0.5899
2024-06-22 20:49:12,555 Epoch [2/8], Batch [1/748], Loss: 0.8094
2024-06-22 20:50:02,946 Epoch [2/8], Batch [51/748], Loss: 0.8103
2024-06-22 20:50:53,703 Epoch [2/8], Batch [101/748], Loss: 0.7713
2024-06-22 20:51:44,757 Epoch [2/8], Batch [151/748], Loss: 0.6464
2024-06-22 20:52:35,902 Epoch [2/8], Batch [201/748], Loss: 0.8992
2024-06-22 20:53:26,509 Epoch [2/8], Batch [251/748], Loss: 0.9922
2024-06-22 20:54:17,189 Epoch [2/8], Batch [301/748], Loss: 0.8226
2024-06-22 20:55:08,215 Epoch [2/8], Batch [351/748], Loss: 0.9929
2024-06-22 20:55:59,182 Epoch [2/8], Batch [401/748], Loss: 0.7691
2024-06-22 20:56:50,048 Epoch [2/8], Batch [451/748], Loss: 0.9862
2024-06-22 20:57:40,947 Epoch [2/8], Batch [501/748], Loss: 0.7730
2024-06-22 20:58:31,521 Epoch [2/8], Batch [551/748], Loss: 0.8713
2024-06-22 20:59:22,091 Epoch [2/8], Batch [601/748], Loss: 0.7044
2024-06-22 21:00:12,723 Epoch [2/8], Batch [651/748], Loss: 0.9105
2024-06-22 21:01:03,621 Epoch [2/8], Batch [701/748], Loss: 0.7801
2024-06-22 21:01:51,083 Epoch 2/8, Train Loss: 0.8421, Train Accuracy: 0.6262
2024-06-22 21:03:16,872 Epoch 2/8, Val Loss: 0.8288, Val Accuracy: 0.6223
2024-06-22 21:03:17,876 Epoch [3/8], Batch [1/748], Loss: 0.7294
2024-06-22 21:04:08,503 Epoch [3/8], Batch [51/748], Loss: 0.8624
2024-06-22 21:04:59,343 Epoch [3/8], Batch [101/748], Loss: 0.7231
2024-06-22 21:05:49,881 Epoch [3/8], Batch [151/748], Loss: 0.5974
2024-06-22 21:06:40,922 Epoch [3/8], Batch [201/748], Loss: 0.6070
2024-06-22 21:07:31,760 Epoch [3/8], Batch [251/748], Loss: 0.6349
2024-06-22 21:08:22,663 Epoch [3/8], Batch [301/748], Loss: 0.6060
2024-06-22 21:09:13,572 Epoch [3/8], Batch [351/748], Loss: 0.7325
2024-06-22 21:10:04,537 Epoch [3/8], Batch [401/748], Loss: 0.5482
2024-06-22 21:10:55,411 Epoch [3/8], Batch [451/748], Loss: 1.0158
2024-06-22 21:11:46,505 Epoch [3/8], Batch [501/748], Loss: 0.7108
2024-06-22 21:12:37,586 Epoch [3/8], Batch [551/748], Loss: 0.6934
2024-06-22 21:13:28,249 Epoch [3/8], Batch [601/748], Loss: 0.6090
2024-06-22 21:14:19,216 Epoch [3/8], Batch [651/748], Loss: 0.6072
2024-06-22 21:15:10,143 Epoch [3/8], Batch [701/748], Loss: 0.7365
2024-06-22 21:15:57,630 Epoch 3/8, Train Loss: 0.7348, Train Accuracy: 0.6686
2024-06-22 21:17:22,801 Epoch 3/8, Val Loss: 0.8685, Val Accuracy: 0.6129
2024-06-22 21:17:23,800 Epoch [4/8], Batch [1/748], Loss: 0.6068
2024-06-22 21:18:14,437 Epoch [4/8], Batch [51/748], Loss: 0.5391
2024-06-22 21:19:05,297 Epoch [4/8], Batch [101/748], Loss: 0.4985
2024-06-22 21:19:55,897 Epoch [4/8], Batch [151/748], Loss: 0.7774
2024-06-22 21:20:46,670 Epoch [4/8], Batch [201/748], Loss: 0.5245
2024-06-22 21:21:37,483 Epoch [4/8], Batch [251/748], Loss: 0.6881
2024-06-22 21:22:28,282 Epoch [4/8], Batch [301/748], Loss: 0.7333
2024-06-22 21:23:19,127 Epoch [4/8], Batch [351/748], Loss: 0.7740
2024-06-22 21:24:10,093 Epoch [4/8], Batch [401/748], Loss: 0.6092
2024-06-22 21:25:00,953 Epoch [4/8], Batch [451/748], Loss: 0.6846
2024-06-22 21:25:51,317 Epoch [4/8], Batch [501/748], Loss: 0.5130
2024-06-22 21:26:41,652 Epoch [4/8], Batch [551/748], Loss: 0.5332
2024-06-22 21:27:31,972 Epoch [4/8], Batch [601/748], Loss: 0.6262
2024-06-22 21:28:22,452 Epoch [4/8], Batch [651/748], Loss: 0.5922
2024-06-22 21:29:12,898 Epoch [4/8], Batch [701/748], Loss: 0.6352
2024-06-22 21:30:00,000 Epoch 4/8, Train Loss: 0.6443, Train Accuracy: 0.7112
2024-06-22 21:31:24,535 Epoch 4/8, Val Loss: 0.7917, Val Accuracy: 0.6541
2024-06-22 21:31:25,512 Epoch [5/8], Batch [1/748], Loss: 0.5786
2024-06-22 21:32:15,829 Epoch [5/8], Batch [51/748], Loss: 0.8054
2024-06-22 21:33:06,469 Epoch [5/8], Batch [101/748], Loss: 0.5456
2024-06-22 21:33:57,073 Epoch [5/8], Batch [151/748], Loss: 0.6017
2024-06-22 21:34:47,650 Epoch [5/8], Batch [201/748], Loss: 0.6658
2024-06-22 21:35:38,336 Epoch [5/8], Batch [251/748], Loss: 0.4467
2024-06-22 21:36:28,838 Epoch [5/8], Batch [301/748], Loss: 0.5541
2024-06-22 21:37:19,296 Epoch [5/8], Batch [351/748], Loss: 0.4231
2024-06-22 21:38:10,039 Epoch [5/8], Batch [401/748], Loss: 0.8195
2024-06-22 21:39:00,727 Epoch [5/8], Batch [451/748], Loss: 0.4402
2024-06-22 21:39:51,130 Epoch [5/8], Batch [501/748], Loss: 0.6561
2024-06-22 21:40:41,742 Epoch [5/8], Batch [551/748], Loss: 0.5046
2024-06-22 21:41:31,979 Epoch [5/8], Batch [601/748], Loss: 0.8131
2024-06-22 21:42:22,831 Epoch [5/8], Batch [651/748], Loss: 0.5155
2024-06-22 21:43:13,835 Epoch [5/8], Batch [701/748], Loss: 0.3690
2024-06-22 21:44:01,003 Epoch 5/8, Train Loss: 0.5547, Train Accuracy: 0.7516
2024-06-22 21:45:25,616 Epoch 5/8, Val Loss: 0.8788, Val Accuracy: 0.6467
2024-06-22 21:45:26,631 Epoch [6/8], Batch [1/748], Loss: 0.6177
2024-06-22 21:46:17,140 Epoch [6/8], Batch [51/748], Loss: 0.7431
2024-06-22 21:47:07,907 Epoch [6/8], Batch [101/748], Loss: 0.4756
2024-06-22 21:47:58,272 Epoch [6/8], Batch [151/748], Loss: 0.6384
2024-06-22 21:48:49,107 Epoch [6/8], Batch [201/748], Loss: 0.4630
2024-06-22 21:49:39,752 Epoch [6/8], Batch [251/748], Loss: 0.4299
2024-06-22 21:50:30,231 Epoch [6/8], Batch [301/748], Loss: 0.4101
2024-06-22 21:51:20,880 Epoch [6/8], Batch [351/748], Loss: 0.3956
2024-06-22 21:52:11,432 Epoch [6/8], Batch [401/748], Loss: 0.4293
2024-06-22 21:53:02,150 Epoch [6/8], Batch [451/748], Loss: 0.7974
2024-06-22 21:53:52,556 Epoch [6/8], Batch [501/748], Loss: 0.6070
2024-06-22 21:54:43,333 Epoch [6/8], Batch [551/748], Loss: 0.3188
2024-06-22 21:55:33,734 Epoch [6/8], Batch [601/748], Loss: 0.5521
2024-06-22 21:56:24,245 Epoch [6/8], Batch [651/748], Loss: 0.5330
2024-06-22 21:57:14,819 Epoch [6/8], Batch [701/748], Loss: 0.3418
2024-06-22 21:58:01,825 Epoch 6/8, Train Loss: 0.4756, Train Accuracy: 0.7874
2024-06-22 21:59:26,330 Epoch 6/8, Val Loss: 0.8503, Val Accuracy: 0.6733
2024-06-22 21:59:27,319 Epoch [7/8], Batch [1/748], Loss: 0.3093
2024-06-22 22:00:17,409 Epoch [7/8], Batch [51/748], Loss: 0.4740
2024-06-22 22:01:08,011 Epoch [7/8], Batch [101/748], Loss: 0.3770
2024-06-22 22:01:58,457 Epoch [7/8], Batch [151/748], Loss: 0.5034
2024-06-22 22:02:49,111 Epoch [7/8], Batch [201/748], Loss: 0.4235
2024-06-22 22:03:39,607 Epoch [7/8], Batch [251/748], Loss: 0.3948
2024-06-22 22:04:29,962 Epoch [7/8], Batch [301/748], Loss: 0.2531
2024-06-22 22:05:20,591 Epoch [7/8], Batch [351/748], Loss: 0.4051
2024-06-22 22:06:11,152 Epoch [7/8], Batch [401/748], Loss: 0.4601
2024-06-22 22:07:01,695 Epoch [7/8], Batch [451/748], Loss: 0.3682
2024-06-22 22:07:51,938 Epoch [7/8], Batch [501/748], Loss: 0.3345
2024-06-22 22:08:42,697 Epoch [7/8], Batch [551/748], Loss: 0.4261
2024-06-22 22:09:33,628 Epoch [7/8], Batch [601/748], Loss: 0.3106
2024-06-22 22:10:24,615 Epoch [7/8], Batch [651/748], Loss: 0.3732
2024-06-22 22:11:15,619 Epoch [7/8], Batch [701/748], Loss: 0.4057
2024-06-22 22:12:02,721 Epoch 7/8, Train Loss: 0.4052, Train Accuracy: 0.8230
2024-06-22 22:13:27,500 Epoch 7/8, Val Loss: 1.0720, Val Accuracy: 0.6385
2024-06-22 22:13:28,491 Epoch [8/8], Batch [1/748], Loss: 0.2777
2024-06-22 22:14:19,079 Epoch [8/8], Batch [51/748], Loss: 0.4043
2024-06-22 22:15:10,086 Epoch [8/8], Batch [101/748], Loss: 0.3026
2024-06-22 22:16:01,022 Epoch [8/8], Batch [151/748], Loss: 0.3793
2024-06-22 22:16:51,998 Epoch [8/8], Batch [201/748], Loss: 0.3344
2024-06-22 22:17:42,797 Epoch [8/8], Batch [251/748], Loss: 0.2465
2024-06-22 22:18:33,254 Epoch [8/8], Batch [301/748], Loss: 0.1797
2024-06-22 22:19:23,708 Epoch [8/8], Batch [351/748], Loss: 0.4606
2024-06-22 22:20:14,109 Epoch [8/8], Batch [401/748], Loss: 0.2922
2024-06-22 22:21:04,784 Epoch [8/8], Batch [451/748], Loss: 0.3769
2024-06-22 22:21:55,550 Epoch [8/8], Batch [501/748], Loss: 0.5814
2024-06-22 22:22:45,962 Epoch [8/8], Batch [551/748], Loss: 0.2023
2024-06-22 22:23:36,669 Epoch [8/8], Batch [601/748], Loss: 0.3635
2024-06-22 22:24:27,603 Epoch [8/8], Batch [651/748], Loss: 0.1689
2024-06-22 22:25:18,503 Epoch [8/8], Batch [701/748], Loss: 0.3789
2024-06-22 22:26:05,758 Epoch 8/8, Train Loss: 0.3545, Train Accuracy: 0.8496
2024-06-22 22:27:30,803 Epoch 8/8, Val Loss: 1.1266, Val Accuracy: 0.6372
2024-06-22 22:27:30,805 Training finished!
2024-06-22 22:27:30,805 ==================================================
2024-06-22 22:27:37,526 ==================================================
2024-06-22 22:27:37,526 Training BERT + TF-IDF features with LR-6e-05...
2024-06-22 22:27:38,561 Epoch [1/8], Batch [1/748], Loss: 1.6266
2024-06-22 22:28:29,125 Epoch [1/8], Batch [51/748], Loss: 1.2438
2024-06-22 22:29:20,086 Epoch [1/8], Batch [101/748], Loss: 1.2121
2024-06-22 22:30:10,966 Epoch [1/8], Batch [151/748], Loss: 1.1260
2024-06-22 22:31:02,113 Epoch [1/8], Batch [201/748], Loss: 1.2578
2024-06-22 22:31:52,830 Epoch [1/8], Batch [251/748], Loss: 1.0718
2024-06-22 22:32:43,829 Epoch [1/8], Batch [301/748], Loss: 1.3139
2024-06-22 22:33:34,744 Epoch [1/8], Batch [351/748], Loss: 0.8941
2024-06-22 22:34:25,701 Epoch [1/8], Batch [401/748], Loss: 0.9013
2024-06-22 22:35:16,452 Epoch [1/8], Batch [451/748], Loss: 1.0616
2024-06-22 22:36:07,453 Epoch [1/8], Batch [501/748], Loss: 0.8511
2024-06-22 22:36:58,321 Epoch [1/8], Batch [551/748], Loss: 1.1854
2024-06-22 22:37:49,263 Epoch [1/8], Batch [601/748], Loss: 0.8938
2024-06-22 22:38:40,300 Epoch [1/8], Batch [651/748], Loss: 1.0206
2024-06-22 22:39:31,157 Epoch [1/8], Batch [701/748], Loss: 0.7840
2024-06-22 22:40:18,603 Epoch 1/8, Train Loss: 1.0381, Train Accuracy: 0.5260
2024-06-22 22:41:43,564 Epoch 1/8, Val Loss: 0.9582, Val Accuracy: 0.5564
2024-06-22 22:41:44,581 Epoch [2/8], Batch [1/748], Loss: 0.8401
2024-06-22 22:42:35,679 Epoch [2/8], Batch [51/748], Loss: 0.7537
2024-06-22 22:43:26,461 Epoch [2/8], Batch [101/748], Loss: 0.8101
2024-06-22 22:44:17,401 Epoch [2/8], Batch [151/748], Loss: 0.6998
2024-06-22 22:45:08,330 Epoch [2/8], Batch [201/748], Loss: 0.8100
2024-06-22 22:45:59,225 Epoch [2/8], Batch [251/748], Loss: 1.0152
2024-06-22 22:46:50,156 Epoch [2/8], Batch [301/748], Loss: 0.6818
2024-06-22 22:47:41,112 Epoch [2/8], Batch [351/748], Loss: 1.1435
2024-06-22 22:48:32,239 Epoch [2/8], Batch [401/748], Loss: 0.6140
2024-06-22 22:49:23,088 Epoch [2/8], Batch [451/748], Loss: 1.0106
2024-06-22 22:50:13,979 Epoch [2/8], Batch [501/748], Loss: 0.7581
2024-06-22 22:51:05,002 Epoch [2/8], Batch [551/748], Loss: 0.6207
2024-06-22 22:51:55,998 Epoch [2/8], Batch [601/748], Loss: 0.8066
2024-06-22 22:52:46,877 Epoch [2/8], Batch [651/748], Loss: 0.7951
2024-06-22 22:53:37,770 Epoch [2/8], Batch [701/748], Loss: 0.6234
2024-06-22 22:54:25,340 Epoch 2/8, Train Loss: 0.8237, Train Accuracy: 0.6346
2024-06-22 22:55:50,394 Epoch 2/8, Val Loss: 0.8798, Val Accuracy: 0.6275
2024-06-22 22:55:51,418 Epoch [3/8], Batch [1/748], Loss: 0.5939
2024-06-22 22:56:42,101 Epoch [3/8], Batch [51/748], Loss: 0.6673
2024-06-22 22:57:33,072 Epoch [3/8], Batch [101/748], Loss: 0.5554
2024-06-22 22:58:23,992 Epoch [3/8], Batch [151/748], Loss: 0.7421
2024-06-22 22:59:14,975 Epoch [3/8], Batch [201/748], Loss: 0.7574
2024-06-22 23:00:05,944 Epoch [3/8], Batch [251/748], Loss: 0.6419
2024-06-22 23:00:57,081 Epoch [3/8], Batch [301/748], Loss: 0.6180
2024-06-22 23:01:47,844 Epoch [3/8], Batch [351/748], Loss: 0.7537
2024-06-22 23:02:38,985 Epoch [3/8], Batch [401/748], Loss: 0.7249
2024-06-22 23:03:29,598 Epoch [3/8], Batch [451/748], Loss: 0.7367
2024-06-22 23:04:20,193 Epoch [3/8], Batch [501/748], Loss: 0.5743
2024-06-22 23:05:11,082 Epoch [3/8], Batch [551/748], Loss: 0.6001
2024-06-22 23:06:01,504 Epoch [3/8], Batch [601/748], Loss: 0.6198
2024-06-22 23:06:52,497 Epoch [3/8], Batch [651/748], Loss: 0.4891
2024-06-22 23:07:43,385 Epoch [3/8], Batch [701/748], Loss: 0.6820
2024-06-22 23:08:30,936 Epoch 3/8, Train Loss: 0.6947, Train Accuracy: 0.6927
2024-06-22 23:09:55,946 Epoch 3/8, Val Loss: 0.7935, Val Accuracy: 0.6638
2024-06-22 23:09:56,969 Epoch [4/8], Batch [1/748], Loss: 0.6020
2024-06-22 23:10:47,721 Epoch [4/8], Batch [51/748], Loss: 0.4343
2024-06-22 23:11:38,664 Epoch [4/8], Batch [101/748], Loss: 0.6933
2024-06-22 23:12:29,784 Epoch [4/8], Batch [151/748], Loss: 0.7896
2024-06-22 23:13:20,584 Epoch [4/8], Batch [201/748], Loss: 0.6200
2024-06-22 23:14:11,499 Epoch [4/8], Batch [251/748], Loss: 0.7443
2024-06-22 23:15:02,525 Epoch [4/8], Batch [301/748], Loss: 0.7389
2024-06-22 23:15:53,462 Epoch [4/8], Batch [351/748], Loss: 0.5212
2024-06-22 23:16:44,345 Epoch [4/8], Batch [401/748], Loss: 0.6889
2024-06-22 23:17:35,256 Epoch [4/8], Batch [451/748], Loss: 0.7498
2024-06-22 23:18:26,269 Epoch [4/8], Batch [501/748], Loss: 0.4943
2024-06-22 23:19:17,235 Epoch [4/8], Batch [551/748], Loss: 0.5830
2024-06-22 23:20:08,142 Epoch [4/8], Batch [601/748], Loss: 0.4593
2024-06-22 23:20:59,014 Epoch [4/8], Batch [651/748], Loss: 0.4739
2024-06-22 23:21:49,957 Epoch [4/8], Batch [701/748], Loss: 0.4567
2024-06-22 23:22:37,673 Epoch 4/8, Train Loss: 0.5759, Train Accuracy: 0.7500
2024-06-22 23:24:02,563 Epoch 4/8, Val Loss: 0.8288, Val Accuracy: 0.6529
2024-06-22 23:24:03,562 Epoch [5/8], Batch [1/748], Loss: 0.4919
2024-06-22 23:24:54,410 Epoch [5/8], Batch [51/748], Loss: 0.3448
2024-06-22 23:25:45,352 Epoch [5/8], Batch [101/748], Loss: 0.3572
2024-06-22 23:26:36,504 Epoch [5/8], Batch [151/748], Loss: 0.6227
2024-06-22 23:27:27,340 Epoch [5/8], Batch [201/748], Loss: 0.3760
2024-06-22 23:28:18,209 Epoch [5/8], Batch [251/748], Loss: 0.3483
2024-06-22 23:29:09,134 Epoch [5/8], Batch [301/748], Loss: 0.6314
2024-06-22 23:29:59,980 Epoch [5/8], Batch [351/748], Loss: 0.2729
2024-06-22 23:30:50,812 Epoch [5/8], Batch [401/748], Loss: 0.6113
2024-06-22 23:31:41,860 Epoch [5/8], Batch [451/748], Loss: 0.3533
2024-06-22 23:32:32,916 Epoch [5/8], Batch [501/748], Loss: 0.4440
2024-06-22 23:33:23,748 Epoch [5/8], Batch [551/748], Loss: 0.4303
2024-06-22 23:34:14,702 Epoch [5/8], Batch [601/748], Loss: 0.2728
2024-06-22 23:35:05,386 Epoch [5/8], Batch [651/748], Loss: 0.3831
2024-06-22 23:35:55,932 Epoch [5/8], Batch [701/748], Loss: 0.2547
2024-06-22 23:36:43,141 Epoch 5/8, Train Loss: 0.4591, Train Accuracy: 0.8061
2024-06-22 23:38:08,293 Epoch 5/8, Val Loss: 0.9161, Val Accuracy: 0.6546
2024-06-22 23:38:09,300 Epoch [6/8], Batch [1/748], Loss: 0.3337
2024-06-22 23:38:59,637 Epoch [6/8], Batch [51/748], Loss: 0.6918
2024-06-22 23:39:50,084 Epoch [6/8], Batch [101/748], Loss: 0.2778
2024-06-22 23:40:40,986 Epoch [6/8], Batch [151/748], Loss: 0.3388
2024-06-22 23:41:31,893 Epoch [6/8], Batch [201/748], Loss: 0.2118
2024-06-22 23:42:22,903 Epoch [6/8], Batch [251/748], Loss: 0.2673
2024-06-22 23:43:13,919 Epoch [6/8], Batch [301/748], Loss: 0.2713
2024-06-22 23:44:04,779 Epoch [6/8], Batch [351/748], Loss: 0.6059
2024-06-22 23:44:55,571 Epoch [6/8], Batch [401/748], Loss: 0.5281
2024-06-22 23:45:46,371 Epoch [6/8], Batch [451/748], Loss: 0.5163
2024-06-22 23:46:37,100 Epoch [6/8], Batch [501/748], Loss: 0.3043
2024-06-22 23:47:27,695 Epoch [6/8], Batch [551/748], Loss: 0.4633
2024-06-22 23:48:18,470 Epoch [6/8], Batch [601/748], Loss: 0.3686
2024-06-22 23:49:09,140 Epoch [6/8], Batch [651/748], Loss: 0.5410
2024-06-22 23:49:59,742 Epoch [6/8], Batch [701/748], Loss: 0.3569
2024-06-22 23:50:47,182 Epoch 6/8, Train Loss: 0.3725, Train Accuracy: 0.8503
2024-06-22 23:52:12,179 Epoch 6/8, Val Loss: 1.0357, Val Accuracy: 0.6588
2024-06-22 23:52:13,204 Epoch [7/8], Batch [1/748], Loss: 0.2456
2024-06-22 23:53:03,943 Epoch [7/8], Batch [51/748], Loss: 0.1225
2024-06-22 23:53:54,658 Epoch [7/8], Batch [101/748], Loss: 0.1008
2024-06-22 23:54:45,476 Epoch [7/8], Batch [151/748], Loss: 0.4141
2024-06-22 23:55:36,521 Epoch [7/8], Batch [201/748], Loss: 0.1556
2024-06-22 23:56:27,239 Epoch [7/8], Batch [251/748], Loss: 0.2067
2024-06-22 23:57:18,225 Epoch [7/8], Batch [301/748], Loss: 0.4083
2024-06-22 23:58:09,113 Epoch [7/8], Batch [351/748], Loss: 0.2221
2024-06-22 23:59:00,099 Epoch [7/8], Batch [401/748], Loss: 0.3178
2024-06-22 23:59:51,025 Epoch [7/8], Batch [451/748], Loss: 0.3550
2024-06-23 00:00:41,859 Epoch [7/8], Batch [501/748], Loss: 0.2587
2024-06-23 00:01:32,464 Epoch [7/8], Batch [551/748], Loss: 0.3262
2024-06-23 00:02:23,209 Epoch [7/8], Batch [601/748], Loss: 0.5397
2024-06-23 00:03:13,781 Epoch [7/8], Batch [651/748], Loss: 0.3673
2024-06-23 00:04:04,620 Epoch [7/8], Batch [701/748], Loss: 0.5440
2024-06-23 00:04:52,210 Epoch 7/8, Train Loss: 0.2917, Train Accuracy: 0.8884
2024-06-23 00:06:17,067 Epoch 7/8, Val Loss: 1.1690, Val Accuracy: 0.6342
2024-06-23 00:06:18,102 Epoch [8/8], Batch [1/748], Loss: 0.5005
2024-06-23 00:07:09,051 Epoch [8/8], Batch [51/748], Loss: 0.1614
2024-06-23 00:07:59,660 Epoch [8/8], Batch [101/748], Loss: 0.1217
2024-06-23 00:08:50,464 Epoch [8/8], Batch [151/748], Loss: 0.0804
2024-06-23 00:09:41,157 Epoch [8/8], Batch [201/748], Loss: 0.1288
2024-06-23 00:10:31,759 Epoch [8/8], Batch [251/748], Loss: 0.1044
2024-06-23 00:11:22,627 Epoch [8/8], Batch [301/748], Loss: 0.1047
2024-06-23 00:12:13,673 Epoch [8/8], Batch [351/748], Loss: 0.0957
2024-06-23 00:13:04,580 Epoch [8/8], Batch [401/748], Loss: 0.0746
2024-06-23 00:13:55,444 Epoch [8/8], Batch [451/748], Loss: 0.3421
2024-06-23 00:14:46,508 Epoch [8/8], Batch [501/748], Loss: 0.3165
2024-06-23 00:15:37,395 Epoch [8/8], Batch [551/748], Loss: 0.7042
2024-06-23 00:16:28,119 Epoch [8/8], Batch [601/748], Loss: 0.3759
2024-06-23 00:17:19,165 Epoch [8/8], Batch [651/748], Loss: 0.5860
2024-06-23 00:18:10,055 Epoch [8/8], Batch [701/748], Loss: 0.2569
2024-06-23 00:18:57,632 Epoch 8/8, Train Loss: 0.2211, Train Accuracy: 0.9210
2024-06-23 00:20:22,315 Epoch 8/8, Val Loss: 1.3880, Val Accuracy: 0.6131
2024-06-23 00:20:22,316 Training finished!
2024-06-23 00:20:22,316 ==================================================
2024-06-23 00:20:29,523 ==================================================
2024-06-23 00:20:29,523 Training BERT + TF-IDF features with LR-0.0001...
2024-06-23 00:20:30,539 Epoch [1/8], Batch [1/748], Loss: 1.6078
2024-06-23 00:21:20,776 Epoch [1/8], Batch [51/748], Loss: 1.3975
2024-06-23 00:22:11,469 Epoch [1/8], Batch [101/748], Loss: 1.3303
2024-06-23 00:23:02,367 Epoch [1/8], Batch [151/748], Loss: 0.8124
2024-06-23 00:23:53,183 Epoch [1/8], Batch [201/748], Loss: 0.9608
2024-06-23 00:24:43,694 Epoch [1/8], Batch [251/748], Loss: 1.1129
2024-06-23 00:25:34,299 Epoch [1/8], Batch [301/748], Loss: 0.9841
2024-06-23 00:26:25,266 Epoch [1/8], Batch [351/748], Loss: 1.1355
2024-06-23 00:27:15,884 Epoch [1/8], Batch [401/748], Loss: 0.9526
2024-06-23 00:28:06,339 Epoch [1/8], Batch [451/748], Loss: 1.1265
2024-06-23 00:28:57,090 Epoch [1/8], Batch [501/748], Loss: 0.9175
2024-06-23 00:29:47,928 Epoch [1/8], Batch [551/748], Loss: 0.9455
2024-06-23 00:30:38,440 Epoch [1/8], Batch [601/748], Loss: 0.8722
2024-06-23 00:31:29,106 Epoch [1/8], Batch [651/748], Loss: 1.1519
2024-06-23 00:32:19,706 Epoch [1/8], Batch [701/748], Loss: 0.9049
2024-06-23 00:33:07,102 Epoch 1/8, Train Loss: 1.0956, Train Accuracy: 0.5080
2024-06-23 00:34:31,771 Epoch 1/8, Val Loss: 0.9306, Val Accuracy: 0.5889
2024-06-23 00:34:32,787 Epoch [2/8], Batch [1/748], Loss: 1.1543
2024-06-23 00:35:23,128 Epoch [2/8], Batch [51/748], Loss: 0.9163
2024-06-23 00:36:13,595 Epoch [2/8], Batch [101/748], Loss: 0.9942
2024-06-23 00:37:04,489 Epoch [2/8], Batch [151/748], Loss: 0.7980
2024-06-23 00:37:55,412 Epoch [2/8], Batch [201/748], Loss: 1.0091
2024-06-23 00:38:46,289 Epoch [2/8], Batch [251/748], Loss: 0.7434
2024-06-23 00:39:36,986 Epoch [2/8], Batch [301/748], Loss: 0.9654
2024-06-23 00:40:27,425 Epoch [2/8], Batch [351/748], Loss: 1.0133
2024-06-23 00:41:18,124 Epoch [2/8], Batch [401/748], Loss: 0.7587
2024-06-23 00:42:09,118 Epoch [2/8], Batch [451/748], Loss: 0.8620
2024-06-23 00:43:00,580 Epoch [2/8], Batch [501/748], Loss: 1.2102
2024-06-23 00:43:51,152 Epoch [2/8], Batch [551/748], Loss: 0.9013
2024-06-23 00:44:41,964 Epoch [2/8], Batch [601/748], Loss: 0.8303
2024-06-23 00:45:32,859 Epoch [2/8], Batch [651/748], Loss: 0.8289
2024-06-23 00:46:23,831 Epoch [2/8], Batch [701/748], Loss: 0.9584
2024-06-23 00:47:11,534 Epoch 2/8, Train Loss: 0.8784, Train Accuracy: 0.6135
2024-06-23 00:48:37,016 Epoch 2/8, Val Loss: 1.0302, Val Accuracy: 0.5650
2024-06-23 00:48:38,029 Epoch [3/8], Batch [1/748], Loss: 0.8074
2024-06-23 00:49:28,693 Epoch [3/8], Batch [51/748], Loss: 1.1603
2024-06-23 00:50:19,342 Epoch [3/8], Batch [101/748], Loss: 0.7035
2024-06-23 00:51:11,155 Epoch [3/8], Batch [151/748], Loss: 0.9494
2024-06-23 00:52:04,176 Epoch [3/8], Batch [201/748], Loss: 0.8446
2024-06-23 00:52:56,548 Epoch [3/8], Batch [251/748], Loss: 0.7464
2024-06-23 00:53:48,474 Epoch [3/8], Batch [301/748], Loss: 0.7270
2024-06-23 00:54:40,601 Epoch [3/8], Batch [351/748], Loss: 0.9164
2024-06-23 00:55:31,906 Epoch [3/8], Batch [401/748], Loss: 0.7527
2024-06-23 00:56:23,240 Epoch [3/8], Batch [451/748], Loss: 0.7145
2024-06-23 00:57:14,829 Epoch [3/8], Batch [501/748], Loss: 0.6997
2024-06-23 00:58:05,849 Epoch [3/8], Batch [551/748], Loss: 0.6065
2024-06-23 00:58:57,577 Epoch [3/8], Batch [601/748], Loss: 0.9269
2024-06-23 00:59:48,452 Epoch [3/8], Batch [651/748], Loss: 0.7312
2024-06-23 01:00:39,675 Epoch [3/8], Batch [701/748], Loss: 0.6066
2024-06-23 01:01:27,557 Epoch 3/8, Train Loss: 0.7775, Train Accuracy: 0.6564
2024-06-23 01:02:54,457 Epoch 3/8, Val Loss: 0.8589, Val Accuracy: 0.6241
2024-06-23 01:02:55,492 Epoch [4/8], Batch [1/748], Loss: 0.6359
2024-06-23 01:03:46,482 Epoch [4/8], Batch [51/748], Loss: 0.5259
2024-06-23 01:04:37,791 Epoch [4/8], Batch [101/748], Loss: 0.5457
2024-06-23 01:05:29,049 Epoch [4/8], Batch [151/748], Loss: 0.7678
2024-06-23 01:06:20,000 Epoch [4/8], Batch [201/748], Loss: 1.0011
2024-06-23 01:07:10,978 Epoch [4/8], Batch [251/748], Loss: 0.8415
2024-06-23 01:08:02,174 Epoch [4/8], Batch [301/748], Loss: 0.8164
2024-06-23 01:08:53,413 Epoch [4/8], Batch [351/748], Loss: 0.6771
2024-06-23 01:09:44,744 Epoch [4/8], Batch [401/748], Loss: 0.5831
2024-06-23 01:10:36,110 Epoch [4/8], Batch [451/748], Loss: 0.7341
2024-06-23 01:11:27,331 Epoch [4/8], Batch [501/748], Loss: 0.5377
2024-06-23 01:12:18,363 Epoch [4/8], Batch [551/748], Loss: 0.7875
2024-06-23 01:13:09,958 Epoch [4/8], Batch [601/748], Loss: 0.6456
2024-06-23 01:14:00,924 Epoch [4/8], Batch [651/748], Loss: 0.8848
2024-06-23 01:14:51,816 Epoch [4/8], Batch [701/748], Loss: 0.5240
2024-06-23 01:15:39,250 Epoch 4/8, Train Loss: 0.6861, Train Accuracy: 0.7002
2024-06-23 01:19:37,291 Epoch [5/8], Batch [151/748], Loss: 0.6539
2024-06-23 01:20:28,036 Epoch [5/8], Batch [201/748], Loss: 0.8653
2024-06-23 01:21:18,852 Epoch [5/8], Batch [251/748], Loss: 0.5768
2024-06-23 01:22:09,736 Epoch [5/8], Batch [301/748], Loss: 0.5737
2024-06-23 01:23:00,735 Epoch [5/8], Batch [351/748], Loss: 0.5329
2024-06-23 01:23:51,665 Epoch [5/8], Batch [401/748], Loss: 0.7369
2024-06-23 01:24:42,739 Epoch [5/8], Batch [451/748], Loss: 0.4194
2024-06-23 01:25:33,562 Epoch [5/8], Batch [501/748], Loss: 0.6291
2024-06-23 01:26:24,784 Epoch [5/8], Batch [551/748], Loss: 0.7096
2024-06-23 01:27:15,530 Epoch [5/8], Batch [601/748], Loss: 0.5806
2024-06-23 01:28:06,337 Epoch [5/8], Batch [651/748], Loss: 0.7664
2024-06-23 01:28:57,697 Epoch [5/8], Batch [701/748], Loss: 0.3950
2024-06-23 01:29:45,330 Epoch 5/8, Train Loss: 0.5969, Train Accuracy: 0.7401
2024-06-23 01:31:10,703 Epoch 5/8, Val Loss: 1.0095, Val Accuracy: 0.6430
2024-06-23 01:31:11,715 Epoch [6/8], Batch [1/748], Loss: 0.5173
2024-06-23 01:32:02,480 Epoch [6/8], Batch [51/748], Loss: 0.4371
2024-06-23 01:32:53,263 Epoch [6/8], Batch [101/748], Loss: 0.6368
2024-06-23 01:33:44,093 Epoch [6/8], Batch [151/748], Loss: 0.3075
2024-06-23 01:34:34,782 Epoch [6/8], Batch [201/748], Loss: 0.4072
2024-06-23 01:35:25,646 Epoch [6/8], Batch [251/748], Loss: 0.6901
2024-06-23 01:36:16,472 Epoch [6/8], Batch [301/748], Loss: 0.5758
2024-06-23 01:37:07,516 Epoch [6/8], Batch [351/748], Loss: 0.3499
2024-06-23 01:37:58,336 Epoch [6/8], Batch [401/748], Loss: 0.3743
2024-06-23 01:38:49,302 Epoch [6/8], Batch [451/748], Loss: 0.5344
2024-06-23 01:39:40,188 Epoch [6/8], Batch [501/748], Loss: 0.3742
2024-06-23 01:40:31,220 Epoch [6/8], Batch [551/748], Loss: 0.4597
2024-06-23 01:41:22,110 Epoch [6/8], Batch [601/748], Loss: 0.4022
2024-06-23 01:42:13,152 Epoch [6/8], Batch [651/748], Loss: 0.4646
2024-06-23 01:43:04,095 Epoch [6/8], Batch [701/748], Loss: 0.4924
2024-06-23 01:43:51,579 Epoch 6/8, Train Loss: 0.5157, Train Accuracy: 0.7792
2024-06-23 01:45:16,896 Epoch 6/8, Val Loss: 1.0806, Val Accuracy: 0.6305
2024-06-23 01:45:17,866 Epoch [7/8], Batch [1/748], Loss: 0.6827
2024-06-23 01:46:08,461 Epoch [7/8], Batch [51/748], Loss: 0.3106
2024-06-23 01:46:59,584 Epoch [7/8], Batch [101/748], Loss: 0.3555
2024-06-23 01:47:50,337 Epoch [7/8], Batch [151/748], Loss: 0.2253
2024-06-23 01:48:41,373 Epoch [7/8], Batch [201/748], Loss: 0.3741
2024-06-23 01:49:32,236 Epoch [7/8], Batch [251/748], Loss: 0.4264
2024-06-23 01:50:23,223 Epoch [7/8], Batch [301/748], Loss: 0.5002
2024-06-23 01:51:14,153 Epoch [7/8], Batch [351/748], Loss: 0.3235
2024-06-23 01:52:05,066 Epoch [7/8], Batch [401/748], Loss: 0.3730
2024-06-23 01:52:56,112 Epoch [7/8], Batch [451/748], Loss: 0.3646
2024-06-23 01:53:46,869 Epoch [7/8], Batch [501/748], Loss: 0.4095
2024-06-23 01:54:37,432 Epoch [7/8], Batch [551/748], Loss: 0.3644
2024-06-23 01:55:27,876 Epoch [7/8], Batch [601/748], Loss: 0.3355
2024-06-23 01:56:18,402 Epoch [7/8], Batch [651/748], Loss: 0.6839
2024-06-23 01:57:08,973 Epoch [7/8], Batch [701/748], Loss: 0.4111
2024-06-23 01:57:56,223 Epoch 7/8, Train Loss: 0.4427, Train Accuracy: 0.8153
2024-06-23 01:59:20,893 Epoch 7/8, Val Loss: 1.0289, Val Accuracy: 0.6340
2024-06-23 01:59:21,912 Epoch [8/8], Batch [1/748], Loss: 0.3732
2024-06-23 02:00:12,106 Epoch [8/8], Batch [51/748], Loss: 0.5398
2024-06-23 02:01:02,835 Epoch [8/8], Batch [101/748], Loss: 0.3765
2024-06-23 02:01:53,180 Epoch [8/8], Batch [151/748], Loss: 0.4586
2024-06-23 02:02:44,173 Epoch [8/8], Batch [201/748], Loss: 0.4338
2024-06-23 02:03:35,087 Epoch [8/8], Batch [251/748], Loss: 0.2162
2024-06-23 02:04:26,122 Epoch [8/8], Batch [301/748], Loss: 0.4360
2024-06-23 02:05:16,852 Epoch [8/8], Batch [351/748], Loss: 0.3028
2024-06-23 02:06:07,179 Epoch [8/8], Batch [401/748], Loss: 0.4013
2024-06-23 02:06:57,842 Epoch [8/8], Batch [451/748], Loss: 0.3018
2024-06-23 02:07:48,785 Epoch [8/8], Batch [501/748], Loss: 0.2776
2024-06-23 02:08:39,565 Epoch [8/8], Batch [551/748], Loss: 0.4306
2024-06-23 02:09:30,163 Epoch [8/8], Batch [601/748], Loss: 0.3256
2024-06-23 02:10:20,775 Epoch [8/8], Batch [651/748], Loss: 0.3298
2024-06-23 02:11:11,624 Epoch [8/8], Batch [701/748], Loss: 0.6035
2024-06-23 02:11:58,920 Epoch 8/8, Train Loss: 0.3837, Train Accuracy: 0.8493
2024-06-23 02:13:23,599 Epoch 8/8, Val Loss: 1.2203, Val Accuracy: 0.6109
2024-06-23 02:13:23,601 Training finished!
2024-06-23 02:13:23,601 ==================================================
