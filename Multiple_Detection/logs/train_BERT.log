2024-06-22 23:13:33,392 ==================================================
2024-06-22 23:13:33,393 Training Pure BERT with LR-1e-06...
2024-06-22 23:13:35,300 Epoch [1/8], Batch [1/748], Loss: 1.6355
2024-06-22 23:14:24,996 Epoch [1/8], Batch [51/748], Loss: 1.5705
2024-06-22 23:15:16,204 Epoch [1/8], Batch [101/748], Loss: 1.6125
2024-06-22 23:16:08,048 Epoch [1/8], Batch [151/748], Loss: 1.5971
2024-06-22 23:17:00,426 Epoch [1/8], Batch [201/748], Loss: 1.5837
2024-06-22 23:17:52,623 Epoch [1/8], Batch [251/748], Loss: 1.5295
2024-06-22 23:18:44,898 Epoch [1/8], Batch [301/748], Loss: 1.5345
2024-06-22 23:19:37,121 Epoch [1/8], Batch [351/748], Loss: 1.4788
2024-06-22 23:20:28,919 Epoch [1/8], Batch [401/748], Loss: 1.3504
2024-06-22 23:21:20,658 Epoch [1/8], Batch [451/748], Loss: 1.4646
2024-06-22 23:22:12,102 Epoch [1/8], Batch [501/748], Loss: 1.3718
2024-06-22 23:23:03,734 Epoch [1/8], Batch [551/748], Loss: 1.3790
2024-06-22 23:23:55,012 Epoch [1/8], Batch [601/748], Loss: 1.2827
2024-06-22 23:24:46,498 Epoch [1/8], Batch [651/748], Loss: 1.3594
2024-06-22 23:25:38,407 Epoch [1/8], Batch [701/748], Loss: 1.3172
2024-06-22 23:26:26,616 Epoch 1/8, Train Loss: 1.4688, Train Accuracy: 0.3881
2024-06-22 23:27:47,843 Epoch 1/8, Val Loss: 1.2638, Val Accuracy: 0.5247
2024-06-22 23:27:48,883 Epoch [2/8], Batch [1/748], Loss: 1.2473
2024-06-22 23:28:40,567 Epoch [2/8], Batch [51/748], Loss: 1.2194
2024-06-22 23:29:32,463 Epoch [2/8], Batch [101/748], Loss: 1.2420
2024-06-22 23:30:24,662 Epoch [2/8], Batch [151/748], Loss: 1.2928
2024-06-22 23:31:16,974 Epoch [2/8], Batch [201/748], Loss: 1.2355
2024-06-22 23:32:09,417 Epoch [2/8], Batch [251/748], Loss: 1.1670
2024-06-22 23:33:01,659 Epoch [2/8], Batch [301/748], Loss: 1.2450
2024-06-22 23:33:53,194 Epoch [2/8], Batch [351/748], Loss: 1.0593
2024-06-22 23:34:44,812 Epoch [2/8], Batch [401/748], Loss: 1.1094
2024-06-22 23:35:36,697 Epoch [2/8], Batch [451/748], Loss: 1.0842
2024-06-22 23:36:28,320 Epoch [2/8], Batch [501/748], Loss: 1.2252
2024-06-22 23:37:20,140 Epoch [2/8], Batch [551/748], Loss: 1.2273
2024-06-22 23:38:12,068 Epoch [2/8], Batch [601/748], Loss: 1.1046
2024-06-22 23:39:03,761 Epoch [2/8], Batch [651/748], Loss: 0.9866
2024-06-22 23:39:55,510 Epoch [2/8], Batch [701/748], Loss: 1.1421
2024-06-22 23:40:44,165 Epoch 2/8, Train Loss: 1.1911, Train Accuracy: 0.5358
2024-06-22 23:42:08,939 Epoch 2/8, Val Loss: 1.1108, Val Accuracy: 0.5596
2024-06-22 23:42:10,008 Epoch [3/8], Batch [1/748], Loss: 1.0894
2024-06-22 23:43:02,310 Epoch [3/8], Batch [51/748], Loss: 1.1043
2024-06-22 23:43:54,746 Epoch [3/8], Batch [101/748], Loss: 1.1349
2024-06-22 23:44:47,127 Epoch [3/8], Batch [151/748], Loss: 1.1973
2024-06-22 23:45:39,460 Epoch [3/8], Batch [201/748], Loss: 1.0056
2024-06-22 23:46:31,745 Epoch [3/8], Batch [251/748], Loss: 1.2035
2024-06-22 23:47:24,097 Epoch [3/8], Batch [301/748], Loss: 1.1358
2024-06-22 23:48:16,386 Epoch [3/8], Batch [351/748], Loss: 1.1025
2024-06-22 23:49:08,688 Epoch [3/8], Batch [401/748], Loss: 1.1570
2024-06-22 23:50:01,159 Epoch [3/8], Batch [451/748], Loss: 1.0914
2024-06-22 23:50:53,378 Epoch [3/8], Batch [501/748], Loss: 1.0090
2024-06-22 23:51:45,689 Epoch [3/8], Batch [551/748], Loss: 0.9959
2024-06-22 23:52:37,827 Epoch [3/8], Batch [601/748], Loss: 1.0179
2024-06-22 23:53:30,172 Epoch [3/8], Batch [651/748], Loss: 0.9135
2024-06-22 23:54:22,442 Epoch [3/8], Batch [701/748], Loss: 0.9208
2024-06-22 23:55:11,312 Epoch 3/8, Train Loss: 1.0609, Train Accuracy: 0.5707
2024-06-22 23:56:35,447 Epoch 3/8, Val Loss: 1.0096, Val Accuracy: 0.5752
2024-06-22 23:56:36,505 Epoch [4/8], Batch [1/748], Loss: 0.9697
2024-06-22 23:57:28,757 Epoch [4/8], Batch [51/748], Loss: 0.8358
2024-06-22 23:58:21,101 Epoch [4/8], Batch [101/748], Loss: 1.0748
2024-06-22 23:59:13,768 Epoch [4/8], Batch [151/748], Loss: 0.6818
2024-06-23 00:00:06,250 Epoch [4/8], Batch [201/748], Loss: 1.0270
2024-06-23 00:00:58,648 Epoch [4/8], Batch [251/748], Loss: 0.8526
2024-06-23 00:01:51,040 Epoch [4/8], Batch [301/748], Loss: 1.1126
2024-06-23 00:02:43,293 Epoch [4/8], Batch [351/748], Loss: 0.8385
2024-06-23 00:03:35,624 Epoch [4/8], Batch [401/748], Loss: 1.0508
2024-06-23 00:04:28,132 Epoch [4/8], Batch [451/748], Loss: 0.9419
2024-06-23 00:05:20,605 Epoch [4/8], Batch [501/748], Loss: 1.0212
2024-06-23 00:06:12,978 Epoch [4/8], Batch [551/748], Loss: 0.9172
2024-06-23 00:07:05,255 Epoch [4/8], Batch [601/748], Loss: 0.9899
2024-06-23 00:07:57,691 Epoch [4/8], Batch [651/748], Loss: 0.9779
2024-06-23 00:08:50,146 Epoch [4/8], Batch [701/748], Loss: 1.0270
2024-06-23 00:09:39,404 Epoch 4/8, Train Loss: 0.9835, Train Accuracy: 0.5910
2024-06-23 00:11:04,557 Epoch 4/8, Val Loss: 0.9705, Val Accuracy: 0.5854
2024-06-23 00:11:05,593 Epoch [5/8], Batch [1/748], Loss: 0.8916
2024-06-23 00:11:58,051 Epoch [5/8], Batch [51/748], Loss: 0.9602
2024-06-23 00:12:50,487 Epoch [5/8], Batch [101/748], Loss: 1.0222
2024-06-23 00:13:43,449 Epoch [5/8], Batch [151/748], Loss: 0.9390
2024-06-23 00:14:36,035 Epoch [5/8], Batch [201/748], Loss: 0.9075
2024-06-23 00:15:28,468 Epoch [5/8], Batch [251/748], Loss: 0.9635
2024-06-23 00:16:20,986 Epoch [5/8], Batch [301/748], Loss: 0.6724
2024-06-23 00:17:13,875 Epoch [5/8], Batch [351/748], Loss: 0.8133
2024-06-23 00:18:06,583 Epoch [5/8], Batch [401/748], Loss: 1.0524
2024-06-23 00:18:59,135 Epoch [5/8], Batch [451/748], Loss: 1.0028
2024-06-23 00:19:51,700 Epoch [5/8], Batch [501/748], Loss: 1.0235
2024-06-23 00:20:44,312 Epoch [5/8], Batch [551/748], Loss: 0.9148
2024-06-23 00:21:36,727 Epoch [5/8], Batch [601/748], Loss: 0.8194
2024-06-23 00:22:29,915 Epoch [5/8], Batch [651/748], Loss: 0.9428
2024-06-23 00:23:22,795 Epoch [5/8], Batch [701/748], Loss: 1.0852
2024-06-23 00:24:11,739 Epoch 5/8, Train Loss: 0.9294, Train Accuracy: 0.6071
2024-06-23 00:25:36,968 Epoch 5/8, Val Loss: 0.9449, Val Accuracy: 0.5944
2024-06-23 00:25:38,016 Epoch [6/8], Batch [1/748], Loss: 1.0854
2024-06-23 00:26:30,120 Epoch [6/8], Batch [51/748], Loss: 0.8843
2024-06-23 00:27:22,871 Epoch [6/8], Batch [101/748], Loss: 0.7659
2024-06-23 00:28:15,653 Epoch [6/8], Batch [151/748], Loss: 0.8745
2024-06-23 00:29:08,493 Epoch [6/8], Batch [201/748], Loss: 0.8117
2024-06-23 00:30:01,192 Epoch [6/8], Batch [251/748], Loss: 0.9501
2024-06-23 00:30:53,898 Epoch [6/8], Batch [301/748], Loss: 0.7341
2024-06-23 00:31:46,383 Epoch [6/8], Batch [351/748], Loss: 0.7594
2024-06-23 00:32:38,865 Epoch [6/8], Batch [401/748], Loss: 0.6852
2024-06-23 00:33:31,458 Epoch [6/8], Batch [451/748], Loss: 0.5889
2024-06-23 00:34:23,969 Epoch [6/8], Batch [501/748], Loss: 0.9266
2024-06-23 00:35:16,687 Epoch [6/8], Batch [551/748], Loss: 0.8039
2024-06-23 00:36:08,949 Epoch [6/8], Batch [601/748], Loss: 1.0195
2024-06-23 00:37:01,339 Epoch [6/8], Batch [651/748], Loss: 1.0295
2024-06-23 00:37:53,920 Epoch [6/8], Batch [701/748], Loss: 0.6771
2024-06-23 00:38:43,391 Epoch 6/8, Train Loss: 0.8865, Train Accuracy: 0.6212
2024-06-23 00:40:09,815 Epoch 6/8, Val Loss: 0.9024, Val Accuracy: 0.6124
2024-06-23 00:40:10,856 Epoch [7/8], Batch [1/748], Loss: 0.8607
2024-06-23 00:41:03,378 Epoch [7/8], Batch [51/748], Loss: 0.8612
2024-06-23 00:41:56,290 Epoch [7/8], Batch [101/748], Loss: 0.8817
2024-06-23 00:42:49,217 Epoch [7/8], Batch [151/748], Loss: 1.0294
2024-06-23 00:43:42,102 Epoch [7/8], Batch [201/748], Loss: 0.8144
2024-06-23 00:44:34,985 Epoch [7/8], Batch [251/748], Loss: 0.8329
2024-06-23 00:45:27,811 Epoch [7/8], Batch [301/748], Loss: 0.8311
2024-06-23 00:46:20,526 Epoch [7/8], Batch [351/748], Loss: 0.6579
2024-06-23 00:47:13,257 Epoch [7/8], Batch [401/748], Loss: 0.9682
2024-06-23 00:48:06,145 Epoch [7/8], Batch [451/748], Loss: 0.7862
2024-06-23 00:48:58,845 Epoch [7/8], Batch [501/748], Loss: 0.9583
2024-06-23 00:49:51,517 Epoch [7/8], Batch [551/748], Loss: 0.8652
2024-06-23 00:50:44,132 Epoch [7/8], Batch [601/748], Loss: 0.8627
2024-06-23 00:51:36,918 Epoch [7/8], Batch [651/748], Loss: 0.6023
2024-06-23 00:52:29,822 Epoch [7/8], Batch [701/748], Loss: 0.7639
2024-06-23 00:53:19,034 Epoch 7/8, Train Loss: 0.8550, Train Accuracy: 0.6278
2024-06-23 00:54:43,862 Epoch 7/8, Val Loss: 0.9350, Val Accuracy: 0.6002
2024-06-23 00:54:44,909 Epoch [8/8], Batch [1/748], Loss: 0.7919
2024-06-23 00:55:37,448 Epoch [8/8], Batch [51/748], Loss: 0.8613
2024-06-23 00:56:29,935 Epoch [8/8], Batch [101/748], Loss: 0.8406
2024-06-23 00:57:22,396 Epoch [8/8], Batch [151/748], Loss: 0.8048
2024-06-23 00:58:15,002 Epoch [8/8], Batch [201/748], Loss: 0.6961
2024-06-23 00:59:07,576 Epoch [8/8], Batch [251/748], Loss: 0.8222
2024-06-23 01:00:00,169 Epoch [8/8], Batch [301/748], Loss: 0.5163
2024-06-23 01:00:52,820 Epoch [8/8], Batch [351/748], Loss: 0.8378
2024-06-23 01:01:45,335 Epoch [8/8], Batch [401/748], Loss: 0.8905
2024-06-23 01:02:37,888 Epoch [8/8], Batch [451/748], Loss: 0.8960
2024-06-23 01:03:30,910 Epoch [8/8], Batch [501/748], Loss: 0.6533
2024-06-23 01:04:23,412 Epoch [8/8], Batch [551/748], Loss: 0.8478
2024-06-23 01:05:15,770 Epoch [8/8], Batch [601/748], Loss: 0.7780
2024-06-23 01:06:08,268 Epoch [8/8], Batch [651/748], Loss: 1.0069
2024-06-23 01:07:00,575 Epoch [8/8], Batch [701/748], Loss: 0.8610
2024-06-23 01:07:49,493 Epoch 8/8, Train Loss: 0.8277, Train Accuracy: 0.6406
2024-06-23 01:09:14,202 Epoch 8/8, Val Loss: 0.9415, Val Accuracy: 0.5942
2024-06-23 01:09:14,204 Training finished!
2024-06-23 01:09:14,205 ==================================================
2024-06-23 01:09:14,567 ==================================================
2024-06-23 01:09:14,567 Training Pure BERT with LR-6e-06...
2024-06-23 01:09:15,608 Epoch [1/8], Batch [1/748], Loss: 1.6358
2024-06-23 01:10:07,622 Epoch [1/8], Batch [51/748], Loss: 1.5727
2024-06-23 01:11:00,232 Epoch [1/8], Batch [101/748], Loss: 1.4393
2024-06-23 01:11:52,901 Epoch [1/8], Batch [151/748], Loss: 1.3394
2024-06-23 01:12:45,420 Epoch [1/8], Batch [201/748], Loss: 1.2230
2024-06-23 01:13:38,114 Epoch [1/8], Batch [251/748], Loss: 1.1633
2024-06-23 01:14:30,768 Epoch [1/8], Batch [301/748], Loss: 1.0849
2024-06-23 01:15:23,204 Epoch [1/8], Batch [351/748], Loss: 1.0439
2024-06-23 01:16:15,638 Epoch [1/8], Batch [401/748], Loss: 1.1072
2024-06-23 01:17:08,020 Epoch [1/8], Batch [451/748], Loss: 1.0446
2024-06-23 01:18:00,567 Epoch [1/8], Batch [501/748], Loss: 0.8353
2024-06-23 01:18:52,720 Epoch [1/8], Batch [551/748], Loss: 1.0123
2024-06-23 01:19:45,234 Epoch [1/8], Batch [601/748], Loss: 0.9151
2024-06-23 01:20:37,345 Epoch [1/8], Batch [651/748], Loss: 0.9929
2024-06-23 01:21:29,813 Epoch [1/8], Batch [701/748], Loss: 0.6525
2024-06-23 01:22:18,986 Epoch 1/8, Train Loss: 1.1361, Train Accuracy: 0.5308
2024-06-23 01:23:44,688 Epoch 1/8, Val Loss: 0.9365, Val Accuracy: 0.6042
2024-06-23 01:23:45,760 Epoch [2/8], Batch [1/748], Loss: 0.9393
2024-06-23 01:24:38,395 Epoch [2/8], Batch [51/748], Loss: 0.8003
2024-06-23 01:25:31,085 Epoch [2/8], Batch [101/748], Loss: 0.8399
2024-06-23 01:26:24,040 Epoch [2/8], Batch [151/748], Loss: 0.8900
2024-06-23 01:27:16,912 Epoch [2/8], Batch [201/748], Loss: 0.9658
2024-06-23 01:28:09,519 Epoch [2/8], Batch [251/748], Loss: 0.7750
2024-06-23 01:29:02,398 Epoch [2/8], Batch [301/748], Loss: 0.8650
2024-06-23 01:29:55,142 Epoch [2/8], Batch [351/748], Loss: 0.8132
2024-06-23 01:30:47,912 Epoch [2/8], Batch [401/748], Loss: 0.9641
2024-06-23 01:31:40,705 Epoch [2/8], Batch [451/748], Loss: 0.7382
2024-06-23 01:32:33,411 Epoch [2/8], Batch [501/748], Loss: 0.8895
2024-06-23 01:33:26,100 Epoch [2/8], Batch [551/748], Loss: 0.8209
2024-06-23 01:34:18,857 Epoch [2/8], Batch [601/748], Loss: 0.8196
2024-06-23 01:35:11,533 Epoch [2/8], Batch [651/748], Loss: 0.8905
2024-06-23 01:36:04,089 Epoch [2/8], Batch [701/748], Loss: 0.7916
2024-06-23 01:36:53,208 Epoch 2/8, Train Loss: 0.8531, Train Accuracy: 0.6257
2024-06-23 01:38:19,067 Epoch 2/8, Val Loss: 0.9252, Val Accuracy: 0.6173
2024-06-23 01:38:20,135 Epoch [3/8], Batch [1/748], Loss: 0.8626
2024-06-23 01:39:13,013 Epoch [3/8], Batch [51/748], Loss: 0.8411
2024-06-23 01:40:05,793 Epoch [3/8], Batch [101/748], Loss: 0.6882
2024-06-23 01:40:58,619 Epoch [3/8], Batch [151/748], Loss: 0.8307
2024-06-23 01:41:51,240 Epoch [3/8], Batch [201/748], Loss: 1.0431
2024-06-23 01:42:43,919 Epoch [3/8], Batch [251/748], Loss: 0.6895
2024-06-23 01:43:36,867 Epoch [3/8], Batch [301/748], Loss: 0.7546
2024-06-23 01:44:29,517 Epoch [3/8], Batch [351/748], Loss: 0.7210
2024-06-23 01:45:22,240 Epoch [3/8], Batch [401/748], Loss: 0.8909
2024-06-23 01:46:14,785 Epoch [3/8], Batch [451/748], Loss: 0.7160
2024-06-23 01:47:07,498 Epoch [3/8], Batch [501/748], Loss: 0.6962
2024-06-23 01:48:00,099 Epoch [3/8], Batch [551/748], Loss: 0.9626
2024-06-23 01:48:52,548 Epoch [3/8], Batch [601/748], Loss: 0.5026
2024-06-23 01:49:45,108 Epoch [3/8], Batch [651/748], Loss: 0.9659
2024-06-23 01:50:37,819 Epoch [3/8], Batch [701/748], Loss: 0.8594
2024-06-23 01:51:26,990 Epoch 3/8, Train Loss: 0.7642, Train Accuracy: 0.6627
2024-06-23 01:52:51,616 Epoch 3/8, Val Loss: 0.8258, Val Accuracy: 0.6499
2024-06-23 01:52:52,654 Epoch [4/8], Batch [1/748], Loss: 0.7012
2024-06-23 01:53:44,981 Epoch [4/8], Batch [51/748], Loss: 0.6312
2024-06-23 01:54:37,198 Epoch [4/8], Batch [101/748], Loss: 0.8093
2024-06-23 01:55:29,683 Epoch [4/8], Batch [151/748], Loss: 0.6195
2024-06-23 01:56:22,166 Epoch [4/8], Batch [201/748], Loss: 0.5600
2024-06-23 01:57:14,693 Epoch [4/8], Batch [251/748], Loss: 0.5524
2024-06-23 01:58:06,591 Epoch [4/8], Batch [301/748], Loss: 0.8668
2024-06-23 01:58:58,473 Epoch [4/8], Batch [351/748], Loss: 0.8314
2024-06-23 01:59:50,895 Epoch [4/8], Batch [401/748], Loss: 0.8947
2024-06-23 02:00:43,028 Epoch [4/8], Batch [451/748], Loss: 0.8425
2024-06-23 02:01:35,103 Epoch [4/8], Batch [501/748], Loss: 0.4880
2024-06-23 02:02:27,117 Epoch [4/8], Batch [551/748], Loss: 0.5949
2024-06-23 02:03:19,124 Epoch [4/8], Batch [601/748], Loss: 0.8687
2024-06-23 02:04:11,191 Epoch [4/8], Batch [651/748], Loss: 0.5741
2024-06-23 02:05:03,144 Epoch [4/8], Batch [701/748], Loss: 0.5942
2024-06-23 02:05:51,804 Epoch 4/8, Train Loss: 0.6875, Train Accuracy: 0.6940
2024-06-23 02:07:16,067 Epoch 4/8, Val Loss: 0.7818, Val Accuracy: 0.6698
2024-06-23 02:07:17,112 Epoch [5/8], Batch [1/748], Loss: 0.4349
2024-06-23 02:08:09,249 Epoch [5/8], Batch [51/748], Loss: 0.6365
2024-06-23 02:09:01,507 Epoch [5/8], Batch [101/748], Loss: 0.5092
2024-06-23 02:09:53,596 Epoch [5/8], Batch [151/748], Loss: 0.4498
2024-06-23 02:10:45,600 Epoch [5/8], Batch [201/748], Loss: 0.6501
2024-06-23 02:11:37,646 Epoch [5/8], Batch [251/748], Loss: 0.6258
2024-06-23 02:12:29,708 Epoch [5/8], Batch [301/748], Loss: 0.6827
2024-06-23 02:13:21,833 Epoch [5/8], Batch [351/748], Loss: 0.8108
2024-06-23 02:14:13,828 Epoch [5/8], Batch [401/748], Loss: 0.5152
2024-06-23 02:15:05,706 Epoch [5/8], Batch [451/748], Loss: 0.5647
2024-06-23 02:15:57,934 Epoch [5/8], Batch [501/748], Loss: 0.6818
2024-06-23 02:16:49,849 Epoch [5/8], Batch [551/748], Loss: 0.5808
2024-06-23 02:17:41,877 Epoch [5/8], Batch [601/748], Loss: 0.5650
2024-06-23 02:18:33,706 Epoch [5/8], Batch [651/748], Loss: 0.4195
2024-06-23 02:19:25,720 Epoch [5/8], Batch [701/748], Loss: 0.4080
2024-06-23 02:20:14,279 Epoch 5/8, Train Loss: 0.6230, Train Accuracy: 0.7259
2024-06-23 02:21:37,751 Epoch 5/8, Val Loss: 0.8386, Val Accuracy: 0.6613
2024-06-23 02:21:38,800 Epoch [6/8], Batch [1/748], Loss: 0.6923
2024-06-23 02:22:30,588 Epoch [6/8], Batch [51/748], Loss: 0.5659
2024-06-23 02:23:22,666 Epoch [6/8], Batch [101/748], Loss: 0.5909
2024-06-23 02:24:14,776 Epoch [6/8], Batch [151/748], Loss: 0.6596
2024-06-23 02:25:06,644 Epoch [6/8], Batch [201/748], Loss: 0.5919
2024-06-23 02:25:58,859 Epoch [6/8], Batch [251/748], Loss: 0.6079
2024-06-23 02:26:50,818 Epoch [6/8], Batch [301/748], Loss: 0.3879
2024-06-23 02:27:42,773 Epoch [6/8], Batch [351/748], Loss: 0.5833
2024-06-23 02:28:34,648 Epoch [6/8], Batch [401/748], Loss: 0.5998
2024-06-23 02:29:26,901 Epoch [6/8], Batch [451/748], Loss: 0.2562
2024-06-23 02:30:18,903 Epoch [6/8], Batch [501/748], Loss: 0.4801
2024-06-23 02:31:10,629 Epoch [6/8], Batch [551/748], Loss: 0.6215
2024-06-23 02:32:02,166 Epoch [6/8], Batch [601/748], Loss: 0.4661
2024-06-23 02:32:53,764 Epoch [6/8], Batch [651/748], Loss: 0.5744
2024-06-23 02:33:45,613 Epoch [6/8], Batch [701/748], Loss: 0.7337
2024-06-23 02:34:33,612 Epoch 6/8, Train Loss: 0.5563, Train Accuracy: 0.7589
2024-06-23 02:35:55,028 Epoch 6/8, Val Loss: 0.8695, Val Accuracy: 0.6581
2024-06-23 02:35:56,064 Epoch [7/8], Batch [1/748], Loss: 0.6259
2024-06-23 02:36:47,280 Epoch [7/8], Batch [51/748], Loss: 0.4080
2024-06-23 02:37:38,761 Epoch [7/8], Batch [101/748], Loss: 0.4334
2024-06-23 02:38:30,337 Epoch [7/8], Batch [151/748], Loss: 0.4981
2024-06-23 02:39:22,387 Epoch [7/8], Batch [201/748], Loss: 0.6093
2024-06-23 02:40:13,956 Epoch [7/8], Batch [251/748], Loss: 0.5670
2024-06-23 02:41:05,579 Epoch [7/8], Batch [301/748], Loss: 0.6694
2024-06-23 02:41:57,215 Epoch [7/8], Batch [351/748], Loss: 0.4100
2024-06-23 02:42:48,728 Epoch [7/8], Batch [401/748], Loss: 0.6514
2024-06-23 02:43:40,470 Epoch [7/8], Batch [451/748], Loss: 0.5744
2024-06-23 02:44:32,232 Epoch [7/8], Batch [501/748], Loss: 0.4812
2024-06-23 02:45:24,030 Epoch [7/8], Batch [551/748], Loss: 0.4173
2024-06-23 02:46:15,483 Epoch [7/8], Batch [601/748], Loss: 0.6386
2024-06-23 02:47:07,043 Epoch [7/8], Batch [651/748], Loss: 0.4348
2024-06-23 02:47:58,652 Epoch [7/8], Batch [701/748], Loss: 0.5468
2024-06-23 02:48:46,748 Epoch 7/8, Train Loss: 0.4946, Train Accuracy: 0.7912
2024-06-23 02:50:08,136 Epoch 7/8, Val Loss: 0.9014, Val Accuracy: 0.6639
2024-06-23 02:50:09,155 Epoch [8/8], Batch [1/748], Loss: 0.4216
2024-06-23 02:51:00,510 Epoch [8/8], Batch [51/748], Loss: 0.4761
2024-06-23 02:51:52,141 Epoch [8/8], Batch [101/748], Loss: 0.3699
2024-06-23 02:52:43,827 Epoch [8/8], Batch [151/748], Loss: 0.3816
2024-06-23 02:53:35,715 Epoch [8/8], Batch [201/748], Loss: 0.3912
2024-06-23 02:54:27,494 Epoch [8/8], Batch [251/748], Loss: 0.6939
2024-06-23 02:55:19,226 Epoch [8/8], Batch [301/748], Loss: 0.6671
2024-06-23 02:56:10,820 Epoch [8/8], Batch [351/748], Loss: 0.3836
2024-06-23 02:57:02,309 Epoch [8/8], Batch [401/748], Loss: 0.2395
2024-06-23 02:57:54,011 Epoch [8/8], Batch [451/748], Loss: 0.3945
2024-06-23 02:58:45,667 Epoch [8/8], Batch [501/748], Loss: 0.2664
2024-06-23 02:59:37,547 Epoch [8/8], Batch [551/748], Loss: 0.5227
2024-06-23 03:00:29,090 Epoch [8/8], Batch [601/748], Loss: 0.2940
2024-06-23 03:01:20,872 Epoch [8/8], Batch [651/748], Loss: 0.4738
2024-06-23 03:02:12,377 Epoch [8/8], Batch [701/748], Loss: 0.6692
2024-06-23 03:03:00,323 Epoch 8/8, Train Loss: 0.4332, Train Accuracy: 0.8209
2024-06-23 03:04:21,885 Epoch 8/8, Val Loss: 0.9066, Val Accuracy: 0.6775
2024-06-23 03:04:21,888 Training finished!
2024-06-23 03:04:21,888 ==================================================
2024-06-23 03:04:22,374 ==================================================
2024-06-23 03:04:22,374 Training Pure BERT with LR-1e-05...
2024-06-23 03:04:23,408 Epoch [1/8], Batch [1/748], Loss: 1.6346
2024-06-23 03:05:14,939 Epoch [1/8], Batch [51/748], Loss: 1.4258
2024-06-23 03:06:06,455 Epoch [1/8], Batch [101/748], Loss: 1.2248
2024-06-23 03:06:57,923 Epoch [1/8], Batch [151/748], Loss: 1.2490
2024-06-23 03:07:49,730 Epoch [1/8], Batch [201/748], Loss: 1.0890
2024-06-23 03:08:41,303 Epoch [1/8], Batch [251/748], Loss: 0.9845
2024-06-23 03:09:32,945 Epoch [1/8], Batch [301/748], Loss: 1.0295
2024-06-23 03:10:24,819 Epoch [1/8], Batch [351/748], Loss: 0.9246
2024-06-23 03:11:16,618 Epoch [1/8], Batch [401/748], Loss: 0.9343
2024-06-23 03:12:08,141 Epoch [1/8], Batch [451/748], Loss: 1.0740
2024-06-23 03:12:59,622 Epoch [1/8], Batch [501/748], Loss: 1.0172
2024-06-23 03:13:51,344 Epoch [1/8], Batch [551/748], Loss: 0.7394
2024-06-23 03:14:43,030 Epoch [1/8], Batch [601/748], Loss: 0.9262
2024-06-23 03:15:34,668 Epoch [1/8], Batch [651/748], Loss: 0.8969
2024-06-23 03:16:26,361 Epoch [1/8], Batch [701/748], Loss: 0.9959
2024-06-23 03:17:14,949 Epoch 1/8, Train Loss: 1.0828, Train Accuracy: 0.5322
2024-06-23 03:18:36,795 Epoch 1/8, Val Loss: 0.9193, Val Accuracy: 0.5967
2024-06-23 03:18:37,802 Epoch [2/8], Batch [1/748], Loss: 0.7806
2024-06-23 03:19:29,443 Epoch [2/8], Batch [51/748], Loss: 0.9164
2024-06-23 03:20:21,126 Epoch [2/8], Batch [101/748], Loss: 0.8325
2024-06-23 03:21:12,793 Epoch [2/8], Batch [151/748], Loss: 0.7286
2024-06-23 03:22:04,370 Epoch [2/8], Batch [201/748], Loss: 0.7612
2024-06-23 03:22:55,960 Epoch [2/8], Batch [251/748], Loss: 0.8335
2024-06-23 03:23:47,847 Epoch [2/8], Batch [301/748], Loss: 0.7063
2024-06-23 03:24:39,484 Epoch [2/8], Batch [351/748], Loss: 0.7464
2024-06-23 03:25:31,208 Epoch [2/8], Batch [401/748], Loss: 0.7355
2024-06-23 03:26:22,854 Epoch [2/8], Batch [451/748], Loss: 0.7496
2024-06-23 03:27:14,416 Epoch [2/8], Batch [501/748], Loss: 0.6247
2024-06-23 03:28:06,172 Epoch [2/8], Batch [551/748], Loss: 0.8240
2024-06-23 03:28:57,911 Epoch [2/8], Batch [601/748], Loss: 0.7690
2024-06-23 03:29:49,439 Epoch [2/8], Batch [651/748], Loss: 0.9843
2024-06-23 03:30:40,860 Epoch [2/8], Batch [701/748], Loss: 0.6478
2024-06-23 03:31:29,077 Epoch 2/8, Train Loss: 0.8151, Train Accuracy: 0.6439
2024-06-23 03:32:50,294 Epoch 2/8, Val Loss: 0.8208, Val Accuracy: 0.6470
2024-06-23 03:32:51,315 Epoch [3/8], Batch [1/748], Loss: 0.6233
2024-06-23 03:33:42,840 Epoch [3/8], Batch [51/748], Loss: 1.0355
2024-06-23 03:34:34,062 Epoch [3/8], Batch [101/748], Loss: 0.6364
2024-06-23 03:35:25,683 Epoch [3/8], Batch [151/748], Loss: 0.5693
2024-06-23 03:36:17,292 Epoch [3/8], Batch [201/748], Loss: 0.6370
2024-06-23 03:37:08,864 Epoch [3/8], Batch [251/748], Loss: 0.7941
2024-06-23 03:38:00,297 Epoch [3/8], Batch [301/748], Loss: 1.0546
2024-06-23 03:38:51,722 Epoch [3/8], Batch [351/748], Loss: 0.6325
2024-06-23 03:39:43,479 Epoch [3/8], Batch [401/748], Loss: 0.8585
2024-06-23 03:40:35,479 Epoch [3/8], Batch [451/748], Loss: 0.4745
2024-06-23 03:41:27,176 Epoch [3/8], Batch [501/748], Loss: 0.6700
2024-06-23 03:42:19,041 Epoch [3/8], Batch [551/748], Loss: 0.6071
2024-06-23 03:43:10,760 Epoch [3/8], Batch [601/748], Loss: 0.7396
2024-06-23 03:44:02,518 Epoch [3/8], Batch [651/748], Loss: 0.4046
2024-06-23 03:44:54,009 Epoch [3/8], Batch [701/748], Loss: 0.8179
2024-06-23 03:45:42,288 Epoch 3/8, Train Loss: 0.6976, Train Accuracy: 0.6973
2024-06-23 03:47:03,886 Epoch 3/8, Val Loss: 0.8251, Val Accuracy: 0.6574
2024-06-23 03:47:04,934 Epoch [4/8], Batch [1/748], Loss: 0.4731
2024-06-23 03:47:56,257 Epoch [4/8], Batch [51/748], Loss: 0.7088
2024-06-23 03:48:47,874 Epoch [4/8], Batch [101/748], Loss: 0.5567
2024-06-23 03:49:39,598 Epoch [4/8], Batch [151/748], Loss: 0.5205
2024-06-23 03:50:31,017 Epoch [4/8], Batch [201/748], Loss: 0.5041
2024-06-23 03:51:22,791 Epoch [4/8], Batch [251/748], Loss: 0.4995
2024-06-23 03:52:14,410 Epoch [4/8], Batch [301/748], Loss: 0.4701
2024-06-23 03:53:06,183 Epoch [4/8], Batch [351/748], Loss: 0.6306
2024-06-23 03:53:57,742 Epoch [4/8], Batch [401/748], Loss: 0.5558
2024-06-23 03:54:49,422 Epoch [4/8], Batch [451/748], Loss: 0.3369
2024-06-23 03:55:40,896 Epoch [4/8], Batch [501/748], Loss: 0.6734
2024-06-23 03:56:32,289 Epoch [4/8], Batch [551/748], Loss: 0.5262
2024-06-23 03:57:23,945 Epoch [4/8], Batch [601/748], Loss: 0.3761
2024-06-23 03:58:15,406 Epoch [4/8], Batch [651/748], Loss: 0.5233
2024-06-23 03:59:07,242 Epoch [4/8], Batch [701/748], Loss: 0.5037
2024-06-23 03:59:55,307 Epoch 4/8, Train Loss: 0.5906, Train Accuracy: 0.7496
2024-06-23 04:01:16,541 Epoch 4/8, Val Loss: 0.8912, Val Accuracy: 0.6609
2024-06-23 04:01:17,564 Epoch [5/8], Batch [1/748], Loss: 0.4562
2024-06-23 04:02:08,786 Epoch [5/8], Batch [51/748], Loss: 0.3245
2024-06-23 04:03:00,456 Epoch [5/8], Batch [101/748], Loss: 0.5673
2024-06-23 04:03:52,289 Epoch [5/8], Batch [151/748], Loss: 0.3894
2024-06-23 04:04:43,911 Epoch [5/8], Batch [201/748], Loss: 0.3114
2024-06-23 04:05:35,609 Epoch [5/8], Batch [251/748], Loss: 0.4094
2024-06-23 04:06:27,106 Epoch [5/8], Batch [301/748], Loss: 0.4019
2024-06-23 04:07:18,883 Epoch [5/8], Batch [351/748], Loss: 0.9939
2024-06-23 04:08:10,431 Epoch [5/8], Batch [401/748], Loss: 0.5705
2024-06-23 04:09:02,065 Epoch [5/8], Batch [451/748], Loss: 0.4262
2024-06-23 04:09:53,650 Epoch [5/8], Batch [501/748], Loss: 0.3737
2024-06-23 04:10:45,053 Epoch [5/8], Batch [551/748], Loss: 0.4531
2024-06-23 04:11:36,693 Epoch [5/8], Batch [601/748], Loss: 0.9031
2024-06-23 04:12:28,329 Epoch [5/8], Batch [651/748], Loss: 0.3830
2024-06-23 04:13:19,943 Epoch [5/8], Batch [701/748], Loss: 0.3066
2024-06-23 04:14:07,892 Epoch 5/8, Train Loss: 0.4801, Train Accuracy: 0.8007
2024-06-23 04:15:29,411 Epoch 5/8, Val Loss: 0.9120, Val Accuracy: 0.6691
2024-06-23 04:15:30,438 Epoch [6/8], Batch [1/748], Loss: 0.1673
2024-06-23 04:16:21,809 Epoch [6/8], Batch [51/748], Loss: 0.1659
2024-06-23 04:17:13,300 Epoch [6/8], Batch [101/748], Loss: 0.5152
2024-06-23 04:18:05,069 Epoch [6/8], Batch [151/748], Loss: 0.4162
2024-06-23 04:18:56,603 Epoch [6/8], Batch [201/748], Loss: 0.3800
2024-06-23 04:19:48,432 Epoch [6/8], Batch [251/748], Loss: 0.3894
2024-06-23 04:20:39,887 Epoch [6/8], Batch [301/748], Loss: 0.6570
2024-06-23 04:21:31,577 Epoch [6/8], Batch [351/748], Loss: 0.5664
2024-06-23 04:22:23,095 Epoch [6/8], Batch [401/748], Loss: 0.3933
2024-06-23 04:23:14,778 Epoch [6/8], Batch [451/748], Loss: 0.3774
2024-06-23 04:24:06,237 Epoch [6/8], Batch [501/748], Loss: 0.2131
2024-06-23 04:24:57,680 Epoch [6/8], Batch [551/748], Loss: 0.6472
2024-06-23 04:25:49,368 Epoch [6/8], Batch [601/748], Loss: 0.4390
2024-06-23 04:26:40,818 Epoch [6/8], Batch [651/748], Loss: 0.2761
2024-06-23 04:27:32,604 Epoch [6/8], Batch [701/748], Loss: 0.2645
2024-06-23 04:28:20,586 Epoch 6/8, Train Loss: 0.3875, Train Accuracy: 0.8449
2024-06-23 04:29:41,974 Epoch 6/8, Val Loss: 1.0603, Val Accuracy: 0.6476
2024-06-23 04:29:42,998 Epoch [7/8], Batch [1/748], Loss: 0.2489
2024-06-23 04:30:34,443 Epoch [7/8], Batch [51/748], Loss: 0.2112
2024-06-23 04:31:26,147 Epoch [7/8], Batch [101/748], Loss: 0.2337
2024-06-23 04:32:17,640 Epoch [7/8], Batch [151/748], Loss: 0.2620
2024-06-23 04:33:09,499 Epoch [7/8], Batch [201/748], Loss: 0.3343
2024-06-23 04:34:01,076 Epoch [7/8], Batch [251/748], Loss: 0.2841
2024-06-23 04:34:52,856 Epoch [7/8], Batch [301/748], Loss: 0.2733
2024-06-23 04:35:44,420 Epoch [7/8], Batch [351/748], Loss: 0.1751
2024-06-23 04:36:35,998 Epoch [7/8], Batch [401/748], Loss: 0.8164
2024-06-23 04:37:27,675 Epoch [7/8], Batch [451/748], Loss: 0.1858
2024-06-23 04:38:19,123 Epoch [7/8], Batch [501/748], Loss: 0.1636
2024-06-23 04:39:10,566 Epoch [7/8], Batch [551/748], Loss: 0.3093
2024-06-23 04:40:02,301 Epoch [7/8], Batch [601/748], Loss: 0.3878
2024-06-23 04:40:53,931 Epoch [7/8], Batch [651/748], Loss: 0.1873
2024-06-23 04:41:45,469 Epoch [7/8], Batch [701/748], Loss: 0.3002
2024-06-23 04:42:33,541 Epoch 7/8, Train Loss: 0.3095, Train Accuracy: 0.8777
2024-06-23 04:43:54,959 Epoch 7/8, Val Loss: 1.0670, Val Accuracy: 0.6676
2024-06-23 04:43:56,000 Epoch [8/8], Batch [1/748], Loss: 0.1312
2024-06-23 04:44:47,226 Epoch [8/8], Batch [51/748], Loss: 0.2136
2024-06-23 04:45:38,918 Epoch [8/8], Batch [101/748], Loss: 0.1646
2024-06-23 04:46:30,395 Epoch [8/8], Batch [151/748], Loss: 0.1162
2024-06-23 04:47:21,881 Epoch [8/8], Batch [201/748], Loss: 0.1742
2024-06-23 04:48:13,495 Epoch [8/8], Batch [251/748], Loss: 0.2119
2024-06-23 04:49:05,226 Epoch [8/8], Batch [301/748], Loss: 0.3147
2024-06-23 04:49:56,805 Epoch [8/8], Batch [351/748], Loss: 0.3254
2024-06-23 04:50:48,217 Epoch [8/8], Batch [401/748], Loss: 0.4049
2024-06-23 04:51:39,947 Epoch [8/8], Batch [451/748], Loss: 0.1695
2024-06-23 04:52:31,777 Epoch [8/8], Batch [501/748], Loss: 0.1772
2024-06-23 04:53:23,457 Epoch [8/8], Batch [551/748], Loss: 0.2080
2024-06-23 04:54:14,985 Epoch [8/8], Batch [601/748], Loss: 0.2285
2024-06-23 04:55:06,698 Epoch [8/8], Batch [651/748], Loss: 0.3334
2024-06-23 04:55:58,176 Epoch [8/8], Batch [701/748], Loss: 0.1375
2024-06-23 04:56:46,254 Epoch 8/8, Train Loss: 0.2413, Train Accuracy: 0.9092
2024-06-23 04:58:07,429 Epoch 8/8, Val Loss: 1.1090, Val Accuracy: 0.6628
2024-06-23 04:58:07,431 Training finished!
2024-06-23 04:58:07,431 ==================================================
2024-06-23 04:58:07,758 ==================================================
2024-06-23 04:58:07,758 Training Pure BERT with LR-6e-05...
2024-06-23 04:58:08,792 Epoch [1/8], Batch [1/748], Loss: 1.6722
2024-06-23 04:59:00,088 Epoch [1/8], Batch [51/748], Loss: 1.3304
2024-06-23 04:59:51,984 Epoch [1/8], Batch [101/748], Loss: 1.0134
2024-06-23 05:00:43,642 Epoch [1/8], Batch [151/748], Loss: 1.1291
2024-06-23 05:01:35,468 Epoch [1/8], Batch [201/748], Loss: 0.9545
2024-06-23 05:02:27,017 Epoch [1/8], Batch [251/748], Loss: 1.0773
2024-06-23 05:03:18,909 Epoch [1/8], Batch [301/748], Loss: 1.1669
2024-06-23 05:04:10,577 Epoch [1/8], Batch [351/748], Loss: 1.0296
2024-06-23 05:05:02,084 Epoch [1/8], Batch [401/748], Loss: 0.8608
2024-06-23 05:05:53,861 Epoch [1/8], Batch [451/748], Loss: 0.8075
2024-06-23 05:06:45,327 Epoch [1/8], Batch [501/748], Loss: 0.9817
2024-06-23 05:07:36,993 Epoch [1/8], Batch [551/748], Loss: 1.0312
2024-06-23 05:08:28,557 Epoch [1/8], Batch [601/748], Loss: 0.6800
2024-06-23 05:09:20,182 Epoch [1/8], Batch [651/748], Loss: 1.1348
2024-06-23 05:10:11,692 Epoch [1/8], Batch [701/748], Loss: 0.6322
2024-06-23 05:10:59,765 Epoch 1/8, Train Loss: 0.9851, Train Accuracy: 0.5702
2024-06-23 05:12:21,021 Epoch 1/8, Val Loss: 0.8833, Val Accuracy: 0.6211
2024-06-23 05:12:22,027 Epoch [2/8], Batch [1/748], Loss: 0.7656
2024-06-23 05:13:13,517 Epoch [2/8], Batch [51/748], Loss: 0.8558
2024-06-23 05:14:05,102 Epoch [2/8], Batch [101/748], Loss: 0.7437
2024-06-23 05:14:56,567 Epoch [2/8], Batch [151/748], Loss: 0.8145
2024-06-23 05:15:48,172 Epoch [2/8], Batch [201/748], Loss: 0.9564
2024-06-23 05:16:39,652 Epoch [2/8], Batch [251/748], Loss: 0.7319
2024-06-23 05:17:31,331 Epoch [2/8], Batch [301/748], Loss: 0.9005
2024-06-23 05:18:22,949 Epoch [2/8], Batch [351/748], Loss: 1.0746
2024-06-23 05:19:14,734 Epoch [2/8], Batch [401/748], Loss: 0.6287
2024-06-23 05:20:06,251 Epoch [2/8], Batch [451/748], Loss: 0.7613
2024-06-23 05:20:57,832 Epoch [2/8], Batch [501/748], Loss: 0.8821
2024-06-23 05:21:49,336 Epoch [2/8], Batch [551/748], Loss: 0.9927
2024-06-23 05:22:40,989 Epoch [2/8], Batch [601/748], Loss: 1.0170
2024-06-23 05:23:32,842 Epoch [2/8], Batch [651/748], Loss: 0.7879
2024-06-23 05:24:24,440 Epoch [2/8], Batch [701/748], Loss: 1.0465
2024-06-23 05:25:12,407 Epoch 2/8, Train Loss: 0.7705, Train Accuracy: 0.6588
2024-06-23 05:26:33,626 Epoch 2/8, Val Loss: 0.8052, Val Accuracy: 0.6614
2024-06-23 05:26:34,638 Epoch [3/8], Batch [1/748], Loss: 0.8194
2024-06-23 05:27:26,044 Epoch [3/8], Batch [51/748], Loss: 0.6684
2024-06-23 05:28:17,622 Epoch [3/8], Batch [101/748], Loss: 0.6807
2024-06-23 05:29:09,380 Epoch [3/8], Batch [151/748], Loss: 0.6619
2024-06-23 05:30:01,066 Epoch [3/8], Batch [201/748], Loss: 0.6530
2024-06-23 05:30:52,593 Epoch [3/8], Batch [251/748], Loss: 0.6542
2024-06-23 05:31:44,268 Epoch [3/8], Batch [301/748], Loss: 0.9620
2024-06-23 05:32:35,693 Epoch [3/8], Batch [351/748], Loss: 0.5184
2024-06-23 05:33:27,604 Epoch [3/8], Batch [401/748], Loss: 0.5749
2024-06-23 05:34:19,177 Epoch [3/8], Batch [451/748], Loss: 0.8345
2024-06-23 05:35:10,899 Epoch [3/8], Batch [501/748], Loss: 0.6169
2024-06-23 05:36:02,486 Epoch [3/8], Batch [551/748], Loss: 0.5813
2024-06-23 05:36:53,941 Epoch [3/8], Batch [601/748], Loss: 0.7599
2024-06-23 05:37:45,415 Epoch [3/8], Batch [651/748], Loss: 0.6257
2024-06-23 05:38:37,081 Epoch [3/8], Batch [701/748], Loss: 0.5809
2024-06-23 05:39:25,192 Epoch 3/8, Train Loss: 0.6202, Train Accuracy: 0.7336
2024-06-23 05:40:46,295 Epoch 3/8, Val Loss: 0.9158, Val Accuracy: 0.6369
2024-06-23 05:40:47,299 Epoch [4/8], Batch [1/748], Loss: 0.5556
2024-06-23 05:41:38,761 Epoch [4/8], Batch [51/748], Loss: 0.3573
2024-06-23 05:42:30,234 Epoch [4/8], Batch [101/748], Loss: 0.7033
2024-06-23 05:43:22,038 Epoch [4/8], Batch [151/748], Loss: 0.3268
2024-06-23 05:44:13,659 Epoch [4/8], Batch [201/748], Loss: 0.5479
2024-06-23 05:45:05,481 Epoch [4/8], Batch [251/748], Loss: 0.5315
2024-06-23 05:45:57,080 Epoch [4/8], Batch [301/748], Loss: 0.3563
2024-06-23 05:46:48,623 Epoch [4/8], Batch [351/748], Loss: 0.5742
2024-06-23 05:47:40,372 Epoch [4/8], Batch [401/748], Loss: 0.4401
2024-06-23 05:48:31,957 Epoch [4/8], Batch [451/748], Loss: 0.3184
2024-06-23 05:49:23,369 Epoch [4/8], Batch [501/748], Loss: 0.4829
2024-06-23 05:50:14,681 Epoch [4/8], Batch [551/748], Loss: 0.3060
2024-06-23 05:51:05,998 Epoch [4/8], Batch [601/748], Loss: 0.4866
2024-06-23 05:51:57,403 Epoch [4/8], Batch [651/748], Loss: 0.3542
2024-06-23 05:52:48,791 Epoch [4/8], Batch [701/748], Loss: 0.8257
2024-06-23 05:53:36,923 Epoch 4/8, Train Loss: 0.4931, Train Accuracy: 0.7913
2024-06-23 05:54:57,466 Epoch 4/8, Val Loss: 0.9743, Val Accuracy: 0.6564
2024-06-23 05:54:58,485 Epoch [5/8], Batch [1/748], Loss: 0.3981
2024-06-23 05:55:49,806 Epoch [5/8], Batch [51/748], Loss: 0.4537
2024-06-23 05:56:41,190 Epoch [5/8], Batch [101/748], Loss: 0.4532
2024-06-23 05:57:32,552 Epoch [5/8], Batch [151/748], Loss: 0.1883
2024-06-23 05:58:23,919 Epoch [5/8], Batch [201/748], Loss: 0.2910
2024-06-23 05:59:15,524 Epoch [5/8], Batch [251/748], Loss: 0.5913
2024-06-23 06:00:06,878 Epoch [5/8], Batch [301/748], Loss: 0.1745
2024-06-23 06:00:58,362 Epoch [5/8], Batch [351/748], Loss: 0.3418
2024-06-23 06:01:49,835 Epoch [5/8], Batch [401/748], Loss: 0.2899
2024-06-23 06:02:41,290 Epoch [5/8], Batch [451/748], Loss: 0.4980
2024-06-23 06:03:32,722 Epoch [5/8], Batch [501/748], Loss: 0.3471
2024-06-23 06:04:24,024 Epoch [5/8], Batch [551/748], Loss: 0.2425
2024-06-23 06:05:15,315 Epoch [5/8], Batch [601/748], Loss: 0.5525
2024-06-23 06:06:06,674 Epoch [5/8], Batch [651/748], Loss: 0.5596
2024-06-23 06:06:58,150 Epoch [5/8], Batch [701/748], Loss: 0.4583
2024-06-23 06:07:46,142 Epoch 5/8, Train Loss: 0.3791, Train Accuracy: 0.8429
2024-06-23 06:09:07,144 Epoch 5/8, Val Loss: 0.9770, Val Accuracy: 0.6611
2024-06-23 06:09:08,160 Epoch [6/8], Batch [1/748], Loss: 0.1003
2024-06-23 06:09:59,387 Epoch [6/8], Batch [51/748], Loss: 0.3471
2024-06-23 06:10:50,816 Epoch [6/8], Batch [101/748], Loss: 0.1937
2024-06-23 06:11:42,457 Epoch [6/8], Batch [151/748], Loss: 0.2013
2024-06-23 06:12:33,701 Epoch [6/8], Batch [201/748], Loss: 0.2937
2024-06-23 06:13:25,248 Epoch [6/8], Batch [251/748], Loss: 0.1181
2024-06-23 06:14:16,632 Epoch [6/8], Batch [301/748], Loss: 0.2828
2024-06-23 06:15:08,106 Epoch [6/8], Batch [351/748], Loss: 0.1368
2024-06-23 06:15:59,390 Epoch [6/8], Batch [401/748], Loss: 0.3430
2024-06-23 06:16:50,842 Epoch [6/8], Batch [451/748], Loss: 0.2562
2024-06-23 06:17:42,285 Epoch [6/8], Batch [501/748], Loss: 0.4448
2024-06-23 06:18:33,711 Epoch [6/8], Batch [551/748], Loss: 0.2639
2024-06-23 06:19:25,182 Epoch [6/8], Batch [601/748], Loss: 0.6384
2024-06-23 06:20:16,633 Epoch [6/8], Batch [651/748], Loss: 0.2782
2024-06-23 06:21:07,982 Epoch [6/8], Batch [701/748], Loss: 0.3301
2024-06-23 06:21:55,908 Epoch 6/8, Train Loss: 0.2868, Train Accuracy: 0.8879
2024-06-23 06:23:16,579 Epoch 6/8, Val Loss: 1.1482, Val Accuracy: 0.6460
2024-06-23 06:23:17,595 Epoch [7/8], Batch [1/748], Loss: 0.5768
2024-06-23 06:24:08,734 Epoch [7/8], Batch [51/748], Loss: 0.5028
2024-06-23 06:25:00,030 Epoch [7/8], Batch [101/748], Loss: 0.2683
2024-06-23 06:25:51,610 Epoch [7/8], Batch [151/748], Loss: 0.2204
2024-06-23 06:26:42,900 Epoch [7/8], Batch [201/748], Loss: 0.0675
2024-06-23 06:27:34,081 Epoch [7/8], Batch [251/748], Loss: 0.1588
2024-06-23 06:28:25,489 Epoch [7/8], Batch [301/748], Loss: 0.3893
2024-06-23 06:29:17,007 Epoch [7/8], Batch [351/748], Loss: 0.3081
2024-06-23 06:30:08,562 Epoch [7/8], Batch [401/748], Loss: 0.1983
2024-06-23 06:30:59,973 Epoch [7/8], Batch [451/748], Loss: 0.1627
2024-06-23 06:31:51,386 Epoch [7/8], Batch [501/748], Loss: 0.5838
2024-06-23 06:32:42,809 Epoch [7/8], Batch [551/748], Loss: 0.1304
2024-06-23 06:33:34,209 Epoch [7/8], Batch [601/748], Loss: 0.1496
2024-06-23 06:34:25,531 Epoch [7/8], Batch [651/748], Loss: 0.1993
2024-06-23 06:35:16,984 Epoch [7/8], Batch [701/748], Loss: 0.2737
2024-06-23 06:36:04,882 Epoch 7/8, Train Loss: 0.2211, Train Accuracy: 0.9172
2024-06-23 06:37:25,391 Epoch 7/8, Val Loss: 1.2816, Val Accuracy: 0.6557
2024-06-23 06:37:26,405 Epoch [8/8], Batch [1/748], Loss: 0.1882
2024-06-23 06:38:17,699 Epoch [8/8], Batch [51/748], Loss: 0.0793
2024-06-23 06:39:08,852 Epoch [8/8], Batch [101/748], Loss: 0.4527
2024-06-23 06:40:00,234 Epoch [8/8], Batch [151/748], Loss: 0.1338
2024-06-23 06:40:51,690 Epoch [8/8], Batch [201/748], Loss: 0.0563
2024-06-23 06:41:43,133 Epoch [8/8], Batch [251/748], Loss: 0.1274
2024-06-23 06:42:34,444 Epoch [8/8], Batch [301/748], Loss: 0.2467
2024-06-23 06:43:25,808 Epoch [8/8], Batch [351/748], Loss: 0.1576
2024-06-23 06:44:17,163 Epoch [8/8], Batch [401/748], Loss: 0.2693
2024-06-23 06:45:08,537 Epoch [8/8], Batch [451/748], Loss: 0.1211
2024-06-23 06:45:59,782 Epoch [8/8], Batch [501/748], Loss: 0.1474
2024-06-23 06:46:51,180 Epoch [8/8], Batch [551/748], Loss: 0.0511
2024-06-23 06:47:42,614 Epoch [8/8], Batch [601/748], Loss: 0.2421
2024-06-23 06:48:33,820 Epoch [8/8], Batch [651/748], Loss: 0.3229
2024-06-23 06:49:25,274 Epoch [8/8], Batch [701/748], Loss: 0.0815
2024-06-23 06:50:13,083 Epoch 8/8, Train Loss: 0.1766, Train Accuracy: 0.9373
2024-06-23 06:51:33,616 Epoch 8/8, Val Loss: 1.3156, Val Accuracy: 0.6494
2024-06-23 06:51:33,618 Training finished!
2024-06-23 06:51:33,618 ==================================================
2024-06-23 06:51:33,945 ==================================================
2024-06-23 06:51:33,945 Training Pure BERT with LR-0.0001...
2024-06-23 06:51:34,961 Epoch [1/8], Batch [1/748], Loss: 1.6247
2024-06-23 06:52:26,070 Epoch [1/8], Batch [51/748], Loss: 1.6263
2024-06-23 06:53:17,444 Epoch [1/8], Batch [101/748], Loss: 1.6401
2024-06-23 06:54:08,710 Epoch [1/8], Batch [151/748], Loss: 1.6380
2024-06-23 06:54:59,946 Epoch [1/8], Batch [201/748], Loss: 1.6124
2024-06-23 06:55:51,256 Epoch [1/8], Batch [251/748], Loss: 1.6164
2024-06-23 06:56:42,267 Epoch [1/8], Batch [301/748], Loss: 1.5959
2024-06-23 06:57:33,561 Epoch [1/8], Batch [351/748], Loss: 1.5790
2024-06-23 06:58:24,680 Epoch [1/8], Batch [401/748], Loss: 1.6140
2024-06-23 06:59:16,064 Epoch [1/8], Batch [451/748], Loss: 1.6174
2024-06-23 07:00:07,404 Epoch [1/8], Batch [501/748], Loss: 1.5947
2024-06-23 07:00:58,679 Epoch [1/8], Batch [551/748], Loss: 1.6179
2024-06-23 07:01:50,008 Epoch [1/8], Batch [601/748], Loss: 1.6075
2024-06-23 07:02:41,213 Epoch [1/8], Batch [651/748], Loss: 1.6290
2024-06-23 07:03:32,666 Epoch [1/8], Batch [701/748], Loss: 1.5868
2024-06-23 07:04:20,435 Epoch 1/8, Train Loss: 1.6149, Train Accuracy: 0.2026
2024-06-23 07:05:40,368 Epoch 1/8, Val Loss: 1.6216, Val Accuracy: 0.2023
2024-06-23 07:05:41,378 Epoch [2/8], Batch [1/748], Loss: 1.6395
2024-06-23 07:06:32,612 Epoch [2/8], Batch [51/748], Loss: 1.6060
2024-06-23 07:07:23,821 Epoch [2/8], Batch [101/748], Loss: 1.5946
2024-06-23 07:08:15,044 Epoch [2/8], Batch [151/748], Loss: 1.6483
2024-06-23 07:09:06,247 Epoch [2/8], Batch [201/748], Loss: 1.6115
2024-06-23 07:09:57,636 Epoch [2/8], Batch [251/748], Loss: 1.6196
2024-06-23 07:10:48,915 Epoch [2/8], Batch [301/748], Loss: 1.6141
2024-06-23 07:11:40,201 Epoch [2/8], Batch [351/748], Loss: 1.6053
2024-06-23 07:12:31,449 Epoch [2/8], Batch [401/748], Loss: 1.6145
2024-06-23 07:13:22,669 Epoch [2/8], Batch [451/748], Loss: 1.6026
2024-06-23 07:14:13,853 Epoch [2/8], Batch [501/748], Loss: 1.5950
2024-06-23 07:15:05,157 Epoch [2/8], Batch [551/748], Loss: 1.5946
2024-06-23 07:15:56,507 Epoch [2/8], Batch [601/748], Loss: 1.6188
2024-06-23 07:16:47,848 Epoch [2/8], Batch [651/748], Loss: 1.5805
2024-06-23 07:17:39,040 Epoch [2/8], Batch [701/748], Loss: 1.6015
2024-06-23 07:18:26,780 Epoch 2/8, Train Loss: 1.6134, Train Accuracy: 0.2022
2024-06-23 07:19:46,812 Epoch 2/8, Val Loss: 1.6095, Val Accuracy: 0.2023
2024-06-23 07:19:47,839 Epoch [3/8], Batch [1/748], Loss: 1.6054
2024-06-23 07:20:38,979 Epoch [3/8], Batch [51/748], Loss: 1.6287
2024-06-23 07:21:30,200 Epoch [3/8], Batch [101/748], Loss: 1.6053
2024-06-23 07:22:21,356 Epoch [3/8], Batch [151/748], Loss: 1.6049
2024-06-23 07:23:12,771 Epoch [3/8], Batch [201/748], Loss: 1.6153
2024-06-23 07:24:03,992 Epoch [3/8], Batch [251/748], Loss: 1.6237
2024-06-23 07:24:55,153 Epoch [3/8], Batch [301/748], Loss: 1.6272
2024-06-23 07:25:46,413 Epoch [3/8], Batch [351/748], Loss: 1.5865
2024-06-23 07:26:37,670 Epoch [3/8], Batch [401/748], Loss: 1.6219
2024-06-23 07:27:28,958 Epoch [3/8], Batch [451/748], Loss: 1.6053
2024-06-23 07:28:20,057 Epoch [3/8], Batch [501/748], Loss: 1.6052
2024-06-23 07:29:11,239 Epoch [3/8], Batch [551/748], Loss: 1.6328
2024-06-23 07:30:02,576 Epoch [3/8], Batch [601/748], Loss: 1.6174
2024-06-23 07:30:53,811 Epoch [3/8], Batch [651/748], Loss: 1.6107
2024-06-23 07:31:45,300 Epoch [3/8], Batch [701/748], Loss: 1.6070
2024-06-23 07:32:33,037 Epoch 3/8, Train Loss: 1.6110, Train Accuracy: 0.2011
2024-06-23 07:33:53,062 Epoch 3/8, Val Loss: 1.6117, Val Accuracy: 0.2023
2024-06-23 07:33:54,084 Epoch [4/8], Batch [1/748], Loss: 1.6193
2024-06-23 07:34:45,088 Epoch [4/8], Batch [51/748], Loss: 1.6009
2024-06-23 07:35:36,216 Epoch [4/8], Batch [101/748], Loss: 1.6059
2024-06-23 07:36:27,525 Epoch [4/8], Batch [151/748], Loss: 1.6043
2024-06-23 07:37:18,676 Epoch [4/8], Batch [201/748], Loss: 1.6085
2024-06-23 07:38:09,910 Epoch [4/8], Batch [251/748], Loss: 1.6195
2024-06-23 07:39:01,275 Epoch [4/8], Batch [301/748], Loss: 1.6194
2024-06-23 07:39:52,364 Epoch [4/8], Batch [351/748], Loss: 1.5969
2024-06-23 07:40:43,537 Epoch [4/8], Batch [401/748], Loss: 1.5964
2024-06-23 07:41:34,963 Epoch [4/8], Batch [451/748], Loss: 1.6012
2024-06-23 07:42:26,250 Epoch [4/8], Batch [501/748], Loss: 1.6055
2024-06-23 07:43:17,728 Epoch [4/8], Batch [551/748], Loss: 1.6135
2024-06-23 07:44:08,928 Epoch [4/8], Batch [601/748], Loss: 1.6323
2024-06-23 07:45:00,031 Epoch [4/8], Batch [651/748], Loss: 1.6014
2024-06-23 07:45:51,469 Epoch [4/8], Batch [701/748], Loss: 1.5940
2024-06-23 07:46:39,290 Epoch 4/8, Train Loss: 1.6107, Train Accuracy: 0.2026
2024-06-23 07:47:59,390 Epoch 4/8, Val Loss: 1.6095, Val Accuracy: 0.2023
2024-06-23 07:48:00,408 Epoch [5/8], Batch [1/748], Loss: 1.6087
2024-06-23 07:48:51,380 Epoch [5/8], Batch [51/748], Loss: 1.6200
2024-06-23 07:49:42,694 Epoch [5/8], Batch [101/748], Loss: 1.6045
2024-06-23 07:50:33,956 Epoch [5/8], Batch [151/748], Loss: 1.6001
2024-06-23 07:51:25,221 Epoch [5/8], Batch [201/748], Loss: 1.6173
2024-06-23 07:52:16,470 Epoch [5/8], Batch [251/748], Loss: 1.6217
2024-06-23 07:53:07,808 Epoch [5/8], Batch [301/748], Loss: 1.6049
2024-06-23 07:53:58,939 Epoch [5/8], Batch [351/748], Loss: 1.6191
2024-06-23 07:54:50,177 Epoch [5/8], Batch [401/748], Loss: 1.6094
2024-06-23 07:55:41,483 Epoch [5/8], Batch [451/748], Loss: 1.6159
2024-06-23 07:56:32,676 Epoch [5/8], Batch [501/748], Loss: 1.6194
2024-06-23 07:57:23,868 Epoch [5/8], Batch [551/748], Loss: 1.6041
2024-06-23 07:58:15,159 Epoch [5/8], Batch [601/748], Loss: 1.6190
2024-06-23 07:59:06,431 Epoch [5/8], Batch [651/748], Loss: 1.6077
2024-06-23 07:59:57,811 Epoch [5/8], Batch [701/748], Loss: 1.6030
2024-06-23 08:00:45,554 Epoch 5/8, Train Loss: 1.6108, Train Accuracy: 0.1986
2024-06-23 08:02:05,470 Epoch 5/8, Val Loss: 1.6094, Val Accuracy: 0.2060
2024-06-23 08:02:06,484 Epoch [6/8], Batch [1/748], Loss: 1.6142
2024-06-23 08:02:57,425 Epoch [6/8], Batch [51/748], Loss: 1.6111
2024-06-23 08:03:48,760 Epoch [6/8], Batch [101/748], Loss: 1.6091
2024-06-23 08:04:39,948 Epoch [6/8], Batch [151/748], Loss: 1.6029
2024-06-23 08:05:31,280 Epoch [6/8], Batch [201/748], Loss: 1.6059
2024-06-23 08:06:22,456 Epoch [6/8], Batch [251/748], Loss: 1.6085
2024-06-23 08:07:13,589 Epoch [6/8], Batch [301/748], Loss: 1.6088
2024-06-23 08:08:04,839 Epoch [6/8], Batch [351/748], Loss: 1.6054
2024-06-23 08:08:56,124 Epoch [6/8], Batch [401/748], Loss: 1.6144
2024-06-23 08:09:47,312 Epoch [6/8], Batch [451/748], Loss: 1.6038
2024-06-23 08:10:38,616 Epoch [6/8], Batch [501/748], Loss: 1.6058
2024-06-23 08:11:29,773 Epoch [6/8], Batch [551/748], Loss: 1.6073
2024-06-23 08:12:21,038 Epoch [6/8], Batch [601/748], Loss: 1.6108
2024-06-23 08:13:12,338 Epoch [6/8], Batch [651/748], Loss: 1.6065
2024-06-23 08:14:03,592 Epoch [6/8], Batch [701/748], Loss: 1.6099
2024-06-23 08:14:51,403 Epoch 6/8, Train Loss: 1.6095, Train Accuracy: 0.2017
2024-06-23 08:16:11,272 Epoch 6/8, Val Loss: 1.6094, Val Accuracy: 0.2023
2024-06-23 08:16:12,270 Epoch [7/8], Batch [1/748], Loss: 1.6104
2024-06-23 08:17:03,398 Epoch [7/8], Batch [51/748], Loss: 1.6106
2024-06-23 08:17:54,494 Epoch [7/8], Batch [101/748], Loss: 1.6115
2024-06-23 08:18:45,770 Epoch [7/8], Batch [151/748], Loss: 1.6118
2024-06-23 08:19:37,088 Epoch [7/8], Batch [201/748], Loss: 1.6142
2024-06-23 08:20:28,367 Epoch [7/8], Batch [251/748], Loss: 1.6102
2024-06-23 08:21:19,562 Epoch [7/8], Batch [301/748], Loss: 1.6068
2024-06-23 08:22:10,836 Epoch [7/8], Batch [351/748], Loss: 1.6085
2024-06-23 08:23:02,115 Epoch [7/8], Batch [401/748], Loss: 1.6121
2024-06-23 08:23:53,445 Epoch [7/8], Batch [451/748], Loss: 1.6075
2024-06-23 08:24:44,661 Epoch [7/8], Batch [501/748], Loss: 1.6110
2024-06-23 08:25:35,907 Epoch [7/8], Batch [551/748], Loss: 1.6137
2024-06-23 08:26:27,094 Epoch [7/8], Batch [601/748], Loss: 1.6137
2024-06-23 08:27:18,256 Epoch [7/8], Batch [651/748], Loss: 1.6094
2024-06-23 08:28:09,600 Epoch [7/8], Batch [701/748], Loss: 1.6027
2024-06-23 08:28:57,342 Epoch 7/8, Train Loss: 1.6095, Train Accuracy: 0.2030
2024-06-23 08:30:17,262 Epoch 7/8, Val Loss: 1.6110, Val Accuracy: 0.1938
2024-06-23 08:30:18,304 Epoch [8/8], Batch [1/748], Loss: 1.6061
2024-06-23 08:31:09,313 Epoch [8/8], Batch [51/748], Loss: 1.6098
2024-06-23 08:32:00,530 Epoch [8/8], Batch [101/748], Loss: 1.6032
2024-06-23 08:32:51,648 Epoch [8/8], Batch [151/748], Loss: 1.6040
2024-06-23 08:33:43,233 Epoch [8/8], Batch [201/748], Loss: 1.6092
2024-06-23 08:34:34,502 Epoch [8/8], Batch [251/748], Loss: 1.6053
2024-06-23 08:35:25,837 Epoch [8/8], Batch [301/748], Loss: 1.6029
2024-06-23 08:36:17,044 Epoch [8/8], Batch [351/748], Loss: 1.6076
2024-06-23 08:37:08,405 Epoch [8/8], Batch [401/748], Loss: 1.6095
2024-06-23 08:37:59,606 Epoch [8/8], Batch [451/748], Loss: 1.6085
2024-06-23 08:38:50,789 Epoch [8/8], Batch [501/748], Loss: 1.6097
2024-06-23 08:39:42,051 Epoch [8/8], Batch [551/748], Loss: 1.6096
2024-06-23 08:40:33,294 Epoch [8/8], Batch [601/748], Loss: 1.6078
2024-06-23 08:41:24,478 Epoch [8/8], Batch [651/748], Loss: 1.6093
2024-06-23 08:42:15,778 Epoch [8/8], Batch [701/748], Loss: 1.6089
2024-06-23 08:43:03,657 Epoch 8/8, Train Loss: 1.6095, Train Accuracy: 0.2005
2024-06-23 08:44:23,746 Epoch 8/8, Val Loss: 1.6098, Val Accuracy: 0.1928
2024-06-23 08:44:23,747 Training finished!
2024-06-23 08:44:23,747 ==================================================
2024-06-23 08:44:24,065 ==================================================
2024-06-23 08:44:24,065 Training Pure BERT with LR-0.0006...
2024-06-23 08:44:25,099 Epoch [1/8], Batch [1/748], Loss: 1.6028
2024-06-23 08:45:16,132 Epoch [1/8], Batch [51/748], Loss: 1.6368
2024-06-23 08:46:07,175 Epoch [1/8], Batch [101/748], Loss: 1.7010
2024-06-23 08:46:58,381 Epoch [1/8], Batch [151/748], Loss: 1.6350
2024-06-23 08:47:49,662 Epoch [1/8], Batch [201/748], Loss: 1.6116
2024-06-23 08:48:39,765 Epoch [1/8], Batch [251/748], Loss: 1.6009
2024-06-23 08:49:29,565 Epoch [1/8], Batch [301/748], Loss: 1.6034
2024-06-23 08:50:18,925 Epoch [1/8], Batch [351/748], Loss: 1.6116
2024-06-23 08:51:08,237 Epoch [1/8], Batch [401/748], Loss: 1.6096
2024-06-23 08:51:57,691 Epoch [1/8], Batch [451/748], Loss: 1.6052
2024-06-23 08:52:47,124 Epoch [1/8], Batch [501/748], Loss: 1.6076
2024-06-23 08:53:36,609 Epoch [1/8], Batch [551/748], Loss: 1.6176
2024-06-23 08:54:26,079 Epoch [1/8], Batch [601/748], Loss: 1.6122
2024-06-23 08:55:15,525 Epoch [1/8], Batch [651/748], Loss: 1.6127
2024-06-23 08:56:05,044 Epoch [1/8], Batch [701/748], Loss: 1.6095
2024-06-23 08:56:51,153 Epoch 1/8, Train Loss: 1.6165, Train Accuracy: 0.1982
2024-06-23 08:58:10,257 Epoch 1/8, Val Loss: 1.6095, Val Accuracy: 0.2060
2024-06-23 08:58:11,250 Epoch [2/8], Batch [1/748], Loss: 1.6086
2024-06-23 08:59:00,493 Epoch [2/8], Batch [51/748], Loss: 1.6106
2024-06-23 08:59:49,936 Epoch [2/8], Batch [101/748], Loss: 1.6138
2024-06-23 09:00:39,465 Epoch [2/8], Batch [151/748], Loss: 1.6110
2024-06-23 09:01:28,879 Epoch [2/8], Batch [201/748], Loss: 1.6077
2024-06-23 09:02:18,256 Epoch [2/8], Batch [251/748], Loss: 1.6105
2024-06-23 09:03:07,754 Epoch [2/8], Batch [301/748], Loss: 1.6102
2024-06-23 09:03:57,101 Epoch [2/8], Batch [351/748], Loss: 1.6119
2024-06-23 09:04:46,524 Epoch [2/8], Batch [401/748], Loss: 1.6089
2024-06-23 09:05:35,970 Epoch [2/8], Batch [451/748], Loss: 1.6099
2024-06-23 09:06:25,334 Epoch [2/8], Batch [501/748], Loss: 1.6084
2024-06-23 09:07:14,761 Epoch [2/8], Batch [551/748], Loss: 1.6063
2024-06-23 09:08:04,048 Epoch [2/8], Batch [601/748], Loss: 1.6093
2024-06-23 09:08:53,384 Epoch [2/8], Batch [651/748], Loss: 1.6119
2024-06-23 09:09:42,739 Epoch [2/8], Batch [701/748], Loss: 1.6112
2024-06-23 09:10:28,703 Epoch 2/8, Train Loss: 1.6096, Train Accuracy: 0.2009
2024-06-23 09:11:47,788 Epoch 2/8, Val Loss: 1.6097, Val Accuracy: 0.1938
2024-06-23 09:11:48,766 Epoch [3/8], Batch [1/748], Loss: 1.6088
2024-06-23 09:12:38,011 Epoch [3/8], Batch [51/748], Loss: 1.6072
2024-06-23 09:13:27,520 Epoch [3/8], Batch [101/748], Loss: 1.6083
2024-06-23 09:14:17,585 Epoch [3/8], Batch [151/748], Loss: 1.6062
2024-06-23 09:15:07,982 Epoch [3/8], Batch [201/748], Loss: 1.6051
2024-06-23 09:15:58,775 Epoch [3/8], Batch [251/748], Loss: 1.6097
2024-06-23 09:16:49,274 Epoch [3/8], Batch [301/748], Loss: 1.6057
2024-06-23 09:17:40,270 Epoch [3/8], Batch [351/748], Loss: 1.6077
2024-06-23 09:18:30,502 Epoch [3/8], Batch [401/748], Loss: 1.6086
2024-06-23 09:19:20,586 Epoch [3/8], Batch [451/748], Loss: 1.6140
2024-06-23 09:20:10,650 Epoch [3/8], Batch [501/748], Loss: 1.6088
2024-06-23 09:21:01,234 Epoch [3/8], Batch [551/748], Loss: 1.6083
2024-06-23 09:21:51,145 Epoch [3/8], Batch [601/748], Loss: 1.6069
2024-06-23 09:22:40,623 Epoch [3/8], Batch [651/748], Loss: 1.6121
2024-06-23 09:23:30,334 Epoch [3/8], Batch [701/748], Loss: 1.6063
2024-06-23 09:24:16,562 Epoch 3/8, Train Loss: 1.6095, Train Accuracy: 0.2007
2024-06-23 09:25:37,182 Epoch 3/8, Val Loss: 1.6097, Val Accuracy: 0.1928
2024-06-23 09:25:38,204 Epoch [4/8], Batch [1/748], Loss: 1.6102
2024-06-23 09:26:28,533 Epoch [4/8], Batch [51/748], Loss: 1.6068
2024-06-23 09:27:18,307 Epoch [4/8], Batch [101/748], Loss: 1.6097
2024-06-23 09:28:07,753 Epoch [4/8], Batch [151/748], Loss: 1.6077
2024-06-23 09:28:57,489 Epoch [4/8], Batch [201/748], Loss: 1.6080
2024-06-23 09:29:46,906 Epoch [4/8], Batch [251/748], Loss: 1.6112
2024-06-23 09:30:36,332 Epoch [4/8], Batch [301/748], Loss: 1.6083
2024-06-23 09:31:25,909 Epoch [4/8], Batch [351/748], Loss: 1.6107
2024-06-23 09:32:15,419 Epoch [4/8], Batch [401/748], Loss: 1.6103
2024-06-23 09:33:04,956 Epoch [4/8], Batch [451/748], Loss: 1.6063
2024-06-23 09:33:54,277 Epoch [4/8], Batch [501/748], Loss: 1.6098
2024-06-23 09:34:43,703 Epoch [4/8], Batch [551/748], Loss: 1.6141
2024-06-23 09:35:33,321 Epoch [4/8], Batch [601/748], Loss: 1.6099
2024-06-23 09:36:22,847 Epoch [4/8], Batch [651/748], Loss: 1.6127
2024-06-23 09:37:12,405 Epoch [4/8], Batch [701/748], Loss: 1.6061
2024-06-23 09:37:58,597 Epoch 4/8, Train Loss: 1.6095, Train Accuracy: 0.1986
2024-06-23 09:39:17,917 Epoch 4/8, Val Loss: 1.6098, Val Accuracy: 0.1928
2024-06-23 09:39:18,900 Epoch [5/8], Batch [1/748], Loss: 1.6072
2024-06-23 09:40:08,107 Epoch [5/8], Batch [51/748], Loss: 1.6107
2024-06-23 09:40:57,483 Epoch [5/8], Batch [101/748], Loss: 1.6092
2024-06-23 09:41:47,042 Epoch [5/8], Batch [151/748], Loss: 1.6100
2024-06-23 09:42:36,508 Epoch [5/8], Batch [201/748], Loss: 1.6066
2024-06-23 09:43:26,038 Epoch [5/8], Batch [251/748], Loss: 1.6126
2024-06-23 09:44:15,818 Epoch [5/8], Batch [301/748], Loss: 1.6085
2024-06-23 09:45:05,744 Epoch [5/8], Batch [351/748], Loss: 1.6062
2024-06-23 09:45:55,531 Epoch [5/8], Batch [401/748], Loss: 1.6137
2024-06-23 09:46:44,836 Epoch [5/8], Batch [451/748], Loss: 1.6102
2024-06-23 09:47:34,362 Epoch [5/8], Batch [501/748], Loss: 1.6118
2024-06-23 09:48:23,815 Epoch [5/8], Batch [551/748], Loss: 1.6086
2024-06-23 09:49:13,325 Epoch [5/8], Batch [601/748], Loss: 1.6044
2024-06-23 09:50:02,827 Epoch [5/8], Batch [651/748], Loss: 1.6072
2024-06-23 09:50:52,189 Epoch [5/8], Batch [701/748], Loss: 1.6089
2024-06-23 09:51:38,524 Epoch 5/8, Train Loss: 1.6094, Train Accuracy: 0.1980
2024-06-23 09:52:57,815 Epoch 5/8, Val Loss: 1.6098, Val Accuracy: 0.1928
2024-06-23 09:52:58,809 Epoch [6/8], Batch [1/748], Loss: 1.6095
2024-06-23 09:53:48,330 Epoch [6/8], Batch [51/748], Loss: 1.6087
2024-06-23 09:54:37,816 Epoch [6/8], Batch [101/748], Loss: 1.6109
2024-06-23 09:55:27,444 Epoch [6/8], Batch [151/748], Loss: 1.6070
2024-06-23 09:56:16,903 Epoch [6/8], Batch [201/748], Loss: 1.6109
2024-06-23 09:57:06,615 Epoch [6/8], Batch [251/748], Loss: 1.6110
2024-06-23 09:57:56,209 Epoch [6/8], Batch [301/748], Loss: 1.6101
2024-06-23 09:58:45,726 Epoch [6/8], Batch [351/748], Loss: 1.6081
2024-06-23 09:59:35,432 Epoch [6/8], Batch [401/748], Loss: 1.6111
2024-06-23 10:00:24,890 Epoch [6/8], Batch [451/748], Loss: 1.6105
2024-06-23 10:01:14,479 Epoch [6/8], Batch [501/748], Loss: 1.6057
2024-06-23 10:02:04,194 Epoch [6/8], Batch [551/748], Loss: 1.6067
2024-06-23 10:02:53,717 Epoch [6/8], Batch [601/748], Loss: 1.6083
2024-06-23 10:03:43,431 Epoch [6/8], Batch [651/748], Loss: 1.6058
2024-06-23 10:04:32,966 Epoch [6/8], Batch [701/748], Loss: 1.6026
2024-06-23 10:05:19,320 Epoch 6/8, Train Loss: 1.6094, Train Accuracy: 0.1992
2024-06-23 10:06:38,971 Epoch 6/8, Val Loss: 1.6098, Val Accuracy: 0.1928
2024-06-23 10:06:39,961 Epoch [7/8], Batch [1/748], Loss: 1.6096
2024-06-23 10:07:29,190 Epoch [7/8], Batch [51/748], Loss: 1.6094
2024-06-23 10:08:18,761 Epoch [7/8], Batch [101/748], Loss: 1.6079
2024-06-23 10:09:08,527 Epoch [7/8], Batch [151/748], Loss: 1.6100
2024-06-23 10:09:58,118 Epoch [7/8], Batch [201/748], Loss: 1.6083
2024-06-23 10:10:47,698 Epoch [7/8], Batch [251/748], Loss: 1.6091
2024-06-23 10:11:37,495 Epoch [7/8], Batch [301/748], Loss: 1.6091
2024-06-23 10:12:27,184 Epoch [7/8], Batch [351/748], Loss: 1.6082
2024-06-23 10:13:16,868 Epoch [7/8], Batch [401/748], Loss: 1.6098
2024-06-23 10:14:06,470 Epoch [7/8], Batch [451/748], Loss: 1.6060
2024-06-23 10:14:56,025 Epoch [7/8], Batch [501/748], Loss: 1.6111
2024-06-23 10:15:45,590 Epoch [7/8], Batch [551/748], Loss: 1.6112
2024-06-23 10:16:35,091 Epoch [7/8], Batch [601/748], Loss: 1.6144
2024-06-23 10:17:24,834 Epoch [7/8], Batch [651/748], Loss: 1.6117
2024-06-23 10:18:14,394 Epoch [7/8], Batch [701/748], Loss: 1.6137
2024-06-23 10:19:00,721 Epoch 7/8, Train Loss: 1.6094, Train Accuracy: 0.2007
2024-06-23 10:20:20,743 Epoch 7/8, Val Loss: 1.6100, Val Accuracy: 0.1928
2024-06-23 10:20:21,732 Epoch [8/8], Batch [1/748], Loss: 1.6069
2024-06-23 10:21:11,173 Epoch [8/8], Batch [51/748], Loss: 1.6125
2024-06-23 10:22:00,750 Epoch [8/8], Batch [101/748], Loss: 1.6121
2024-06-23 10:22:50,197 Epoch [8/8], Batch [151/748], Loss: 1.6126
2024-06-23 10:23:40,098 Epoch [8/8], Batch [201/748], Loss: 1.6078
2024-06-23 10:24:29,754 Epoch [8/8], Batch [251/748], Loss: 1.6132
2024-06-23 10:25:19,321 Epoch [8/8], Batch [301/748], Loss: 1.6065
2024-06-23 10:26:08,969 Epoch [8/8], Batch [351/748], Loss: 1.6114
2024-06-23 10:26:58,614 Epoch [8/8], Batch [401/748], Loss: 1.6034
2024-06-23 10:27:48,530 Epoch [8/8], Batch [451/748], Loss: 1.6117
2024-06-23 10:28:38,257 Epoch [8/8], Batch [501/748], Loss: 1.6090
2024-06-23 10:29:27,867 Epoch [8/8], Batch [551/748], Loss: 1.6108
2024-06-23 10:30:17,476 Epoch [8/8], Batch [601/748], Loss: 1.6046
2024-06-23 10:31:07,090 Epoch [8/8], Batch [651/748], Loss: 1.6102
2024-06-23 10:31:56,858 Epoch [8/8], Batch [701/748], Loss: 1.6133
2024-06-23 10:32:42,988 Epoch 8/8, Train Loss: 1.6094, Train Accuracy: 0.2009
2024-06-23 10:34:02,936 Epoch 8/8, Val Loss: 1.6098, Val Accuracy: 0.1938
2024-06-23 10:34:02,938 Training finished!
2024-06-23 10:34:02,938 ==================================================
